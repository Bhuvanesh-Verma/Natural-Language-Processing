{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 12896\n",
      "Test data: 3250\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def read_hate_tweets (annofile, jsonfile):\n",
    "    \"\"\"Reads in hate speech data.\"\"\"\n",
    "    all_data = {}\n",
    "    annos = {}\n",
    "    with open(annofile) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            if row[0] in annos:\n",
    "                # if duplicate with different rating, remove!\n",
    "                if row[1] != annos[row[0]]:\n",
    "                    del(annos[row[0]])\n",
    "            else:\n",
    "                annos[row[0]] = row[1]\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "                \n",
    "    with open(jsonfile) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            twtjson = json.loads(line)\n",
    "            twt_id = twtjson['id_str']\n",
    "            if twt_id in annos:\n",
    "                all_data[twt_id] = {}\n",
    "                all_data[twt_id]['offensive'] = \"nonoffensive\" if annos[twt_id] == 'none' else \"offensive\"\n",
    "                all_data[twt_id]['text_tok'] = tknzr.tokenize(twtjson['text'])\n",
    "\n",
    "    # split training and test data:\n",
    "    all_data_sorted = sorted(all_data.items())\n",
    "    items = [(i[1]['text_tok'],i[1]['offensive']) for i in all_data_sorted]\n",
    "    splititem = len(all_data)-3250\n",
    "    train_dt = items[:splititem]\n",
    "    test_dt = items[splititem:]\n",
    "    print('Training data:',len(train_dt))\n",
    "    print('Test data:',len(test_dt))\n",
    "\n",
    "    return(train_dt,test_dt)\n",
    "\n",
    "TWEETS_ANNO = '../Data/NAACL_SRW_2016.csv'\n",
    "TWEETS_TEXT = '../Data/NAACL_SRW_2016_tweets.json'\n",
    "\n",
    "(train_data,test_data) = read_hate_tweets(TWEETS_ANNO,TWEETS_TEXT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    C=[] # Classes : It is list of classes in dataset\n",
    "    V=[] # Vocabulary : It is list of unique tokens in dataset\n",
    "    log_prior = {} # It stores log_prior wrt to class, eg: {'class1':value,'class2':value,....}\n",
    "    log_likelihood = {} # It stores log_likelihood wrt to class, eg: {'class1':{word1:value,word2:value,..},'class2':{},....}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tp = 0 # True Positive \n",
    "        self.tn = 0 # True Negative\n",
    "        self.fp = 0 # False Positive\n",
    "        self.fn = 0 # False Negative\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        \"\"\"Predicts the class for a document.\n",
    "\n",
    "        Args:\n",
    "            x: A document, represented as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class, represented as a string.\n",
    "        \"\"\"\n",
    "        \n",
    "        sum_ = {} # Variable to store sum of log probability as { 'class1': value, 'class2': value , ....}\n",
    "        \n",
    "        x = remove_unknown(self.V,x) # removes words which are not present in Vocabulary but in test document\n",
    "        \n",
    "        \n",
    "        #x = [word.lower() for word in x] \n",
    "        \n",
    "        \n",
    "        for cl in self.C:\n",
    "            tot = self.log_prior[cl]\n",
    "            for word in x:\n",
    "                if word in self.V:\n",
    "                    tot=tot+self.log_likelihood[cl][word]\n",
    "            sum_[cl] = tot\n",
    "            tot=0\n",
    "        \n",
    "        if(len(sum_)>0):\n",
    "            return max(sum_.items(), key=operator.itemgetter(1))[0]\n",
    "        else:\n",
    "            return None #Returns None if remove_unkown() returns a empty list\n",
    "        \n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def train(cls, data, k=1):\n",
    "        \"\"\"Train a new classifier on training data using maximum\n",
    "        likelihood estimation and additive smoothing.\n",
    "\n",
    "        Args:\n",
    "            cls: The Python class representing the classifier.\n",
    "            data: Training data.\n",
    "            k: The smoothing constant.\n",
    "\n",
    "        Returns:\n",
    "            A trained classifier, an instance of `cls`.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_of_docs = len(data) # total number of tweets in our case\n",
    "        cls.C = set([word[1] for word in data]) # set of classes\n",
    "        cls.V = vocab(data) # set of unique words in training data\n",
    "        \n",
    "        \n",
    "        #cls.V = remove_stop_words(cls.V)\n",
    "        #cls.V = [word.lower() for word in cls.V]\n",
    "        \n",
    "        \n",
    "        words_in_class = {} # It is dictionary which contain tokens and vocabulary wrt class\n",
    "                            # eg: { 'class1' : [[list of token],[list of unique tokens or vocabulary]], 'class2':[[],[]],...}\n",
    "        for cl in cls.C:\n",
    "            num_of_docs_in_c = count_for_class(data,cl)\n",
    "            cls.log_prior[cl] = math.log(num_of_docs_in_c/num_of_docs)\n",
    "            words_in_class[cl] = (vocab_for_class(data,cl))\n",
    "            \n",
    "            \n",
    "            #words_in_class[cl][0] = remove_stop_words(words_in_class[cl][0])\n",
    "            #words_in_class[cl][1] = remove_stop_words(list(words_in_class[cl][1]))\n",
    "            #words_in_class[cl][0] = [word.lower() for word in words_in_class[cl][0]]\n",
    "            #words_in_class[cl][1] = [word.lower() for word in words_in_class[cl][1]]\n",
    "            \n",
    "            \n",
    "            count = {}\n",
    "            cls.log_likelihood[cl] = {}\n",
    "            for word in cls.V:\n",
    "                count[word] = words_in_class[cl][0].count(word)\n",
    "                vc_class_doc = len(words_in_class[cl][0]) + k*len(cls.V)\n",
    "                cls.log_likelihood[cl][word] = math.log((count[word]+k)/(vc_class_doc))\n",
    "        return cls()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def count_for_class(data,class_name):\n",
    "    '''\n",
    "    This function returns number of docs(tweets) with respect\n",
    "    to a given input class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : List of docs with class\n",
    "        DESCRIPTION.\n",
    "    class_name : string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    count_of_class : int \n",
    "\n",
    "    '''\n",
    "    count_of_class = 0\n",
    "    for datum in data:\n",
    "        if(datum[1] == class_name):\n",
    "            count_of_class+=1\n",
    "    return count_of_class\n",
    "\n",
    "\n",
    "\n",
    "def vocab_for_class(data,class_name):\n",
    "    '''\n",
    "    This function creates vocabulary for a particular class and returns\n",
    "    a list of words in a class and set of unique words in that class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : List of docs with class\n",
    "        DESCRIPTION.\n",
    "    class_name : string.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        \n",
    "\n",
    "    '''\n",
    "    vocab = []\n",
    "    for word in data:\n",
    "        if(word[1] == class_name):\n",
    "            vocab+=word[0]\n",
    "    return [vocab,set(vocab)]\n",
    "\n",
    "\n",
    "def vocab(data):\n",
    "    '''\n",
    "    This function creates vocabulary set for a given Document\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list of documents with labelled classes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    set of unique words in set of Documents\n",
    "\n",
    "    '''\n",
    "    vocab = []\n",
    "    for word in data:\n",
    "        vocab+=word[0]\n",
    "    return set(vocab)\n",
    "\n",
    "def remove_unknown(train_vocab,test_vocab):\n",
    "    '''\n",
    "    It removes the words which are present in testing set but not\n",
    "    in training set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_vocab : vocabulary of training set\n",
    "    test_vocab : vocabulary of testing set\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_vocab : modified testing vocabulary\n",
    "\n",
    "    '''\n",
    "    x = set(test_vocab)\n",
    "    x = set(train_vocab).intersection(x)\n",
    "    for word in test_vocab:\n",
    "        if(word not in x):\n",
    "            test_vocab = list(filter(lambda w: w != word, test_vocab))\n",
    "    return test_vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv,math,string\n",
    "import json,operator,random  \n",
    "def accuracy(classifier, data):\n",
    "    \"\"\"Computes the accuracy of a classifier on reference data.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        classifier: A classifier.\n",
    "        data: Reference data.\n",
    "\n",
    "    Returns:\n",
    "        The accuracy of the classifier on the test data, a float.\n",
    "    \"\"\"\n",
    "    \n",
    "    for d in data:\n",
    "        tweet = d[0]\n",
    "        actual = d[1]\n",
    "        predicted = classifier.predict(tweet)\n",
    "        if ( predicted is None):\n",
    "            continue \n",
    "           \n",
    "        # Some document(tweets) might be empty after removing unknown and stop words, in that case predict() returns None\n",
    "        # and continue to next document(tweet)\n",
    "        \n",
    "        if(actual == predicted):\n",
    "            if(predicted == 'offensive'):\n",
    "                classifier.tp+=1\n",
    "            else:\n",
    "                classifier.tn+=1\n",
    "        else:\n",
    "            if(predicted == 'offensive'):\n",
    "                classifier.fp+=1\n",
    "            else:\n",
    "                classifier.fn+=1\n",
    "    return (classifier.tp+classifier.tn)/(classifier.tp+classifier.tn+classifier.fn+classifier.fp)\n",
    "\n",
    "def f_1(classifier, data):\n",
    "    \"\"\"\n",
    "     Computes the F_1-score of a classifier on reference data.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        classifier: A classifier.\n",
    "        data: Reference data.\n",
    "\n",
    "    Returns:\n",
    "        The F_1-score of the classifier on the test data, a float.\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "   \n",
    "    precision = classifier.tp/(classifier.tp+classifier.fp)\n",
    "    recall  = classifier.tp/(classifier.tp+classifier.fn)\n",
    "    f1 = (2*precision*recall)/(precision+recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [] # accuracy list\n",
    "f1 = [] #f1 score list\n",
    "klist = [i/10 for i in range(1,100)] # list of k values ranges 0 to 1 with step of 0.01\n",
    "for k in klist:\n",
    "    nb = NaiveBayes.train(train_data,k)\n",
    "    acc.append(accuracy(nb, test_data))\n",
    "    f1.append(f_1(nb,test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(acc,f1)\n",
    "plt.ylabel('F_1 score')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.show()\n",
    "plt.plot(klist,f1)\n",
    "plt.ylabel('F_1 score')\n",
    "plt.xlabel('Laplace Smoothing Constant (k)')\n",
    "plt.show()\n",
    "plt.plot(klist,acc)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Laplace Smoothing Constant (k)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
