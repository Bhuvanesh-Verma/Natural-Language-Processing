{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 12896\n",
      "Test data: 3250\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def read_hate_tweets (annofile, jsonfile):\n",
    "    \"\"\"Reads in hate speech data.\"\"\"\n",
    "    all_data = {}\n",
    "    annos = {}\n",
    "    with open(annofile) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            if row[0] in annos:\n",
    "                # if duplicate with different rating, remove!\n",
    "                if row[1] != annos[row[0]]:\n",
    "                    del(annos[row[0]])\n",
    "            else:\n",
    "                annos[row[0]] = row[1]\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "                \n",
    "    with open(jsonfile) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            twtjson = json.loads(line)\n",
    "            twt_id = twtjson['id_str']\n",
    "            if twt_id in annos:\n",
    "                all_data[twt_id] = {}\n",
    "                all_data[twt_id]['offensive'] = \"nonoffensive\" if annos[twt_id] == 'none' else \"offensive\"\n",
    "                all_data[twt_id]['text_tok'] = tknzr.tokenize(twtjson['text'])\n",
    "\n",
    "    # split training and test data:\n",
    "    all_data_sorted = sorted(all_data.items())\n",
    "    items = [(i[1]['text_tok'],i[1]['offensive']) for i in all_data_sorted]\n",
    "    splititem = len(all_data)-3250\n",
    "    train_dt = items[:splititem]\n",
    "    test_dt = items[splititem:]\n",
    "    print('Training data:',len(train_dt))\n",
    "    print('Test data:',len(test_dt))\n",
    "\n",
    "    return(train_dt,test_dt)\n",
    "\n",
    "TWEETS_ANNO = '../Data/NAACL_SRW_2016.csv'\n",
    "TWEETS_TEXT = '../Data/NAACL_SRW_2016_tweets.json'\n",
    "\n",
    "(train_data,test_data) = read_hate_tweets(TWEETS_ANNO,TWEETS_TEXT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    C=[]\n",
    "    V=[]\n",
    "    log_prior = {}\n",
    "    log_likelihood = {}\n",
    "    Bi = {}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tp = 0\n",
    "        self.tn = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        \"\"\"Predicts the class for a document.\n",
    "\n",
    "        Args:\n",
    "            x: A document, represented as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class, represented as a string.\n",
    "        \"\"\"\n",
    "        sum_ = {}\n",
    "        x = remove_unknown(self.V,x)\n",
    "        x = [word.lower() for word in x]\n",
    "        y=x \n",
    "        x.insert(0,\"<s>\")\n",
    "        x.insert(len(x),\"</s>\")\n",
    "        bigrams_in_x = [(x[i],x[i+1]) for i in range(len(x)-1)]\n",
    "\n",
    "        for cl in self.C:\n",
    "            tot = self.log_prior[cl]\n",
    "            for b in bigrams_in_x:\n",
    "                if b in list(self.Bi.keys()) :\n",
    "                    tot= tot + self.log_likelihood[cl]['bigram'][b]\n",
    "            for word in y:   \n",
    "                if word in neg_vocab:\n",
    "                    tot= tot + self.log_likelihood[cl]['neg_word'][word]   \n",
    "                    \n",
    "            sum_[cl] = tot\n",
    "            tot = 0\n",
    "        \n",
    "        if(len(sum_)>0):\n",
    "            return max(sum_.items(), key=operator.itemgetter(1))[0]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def train(cls, data, k=1):\n",
    "        \"\"\"Train a new classifier on training data using maximum\n",
    "        likelihood estimation and additive smoothing.\n",
    "\n",
    "        Args:\n",
    "            cls: The Python class representing the classifier.\n",
    "            data: Training data.\n",
    "            k: The smoothing constant.\n",
    "\n",
    "        Returns:\n",
    "            A trained classifier, an instance of `cls`.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_of_docs = len(data) # total number of tweets in our case\n",
    "        cls.C = set([word[1] for word in data]) # set of classes\n",
    "        cls.V = vocab(data) # set of unique words in training data\n",
    "        cls.V = remove_stop_words(cls.V)\n",
    "        cls.V = set([word.lower() for word in cls.V])\n",
    "        words_in_class = {}\n",
    "        bigram = bigrams(data,list(cls.C))\n",
    "        (uniq_bigram,sorted_bigram) = bigrams_info(bigram,cls.C)\n",
    "        cls.Bi = uniq_bigram\n",
    "        for cl in cls.C:\n",
    "            num_of_docs_in_c = count_for_class(data,cl)\n",
    "            cls.log_prior[cl] = math.log(num_of_docs_in_c/num_of_docs)\n",
    "            words_in_class[cl] = (vocab_for_class(data,cl))\n",
    "            words_in_class[cl][0] = remove_stop_words(words_in_class[cl][0])\n",
    "            words_in_class[cl][1] = remove_stop_words(list(words_in_class[cl][1]))\n",
    "            words_in_class[cl][0] = [word.lower() for word in words_in_class[cl][0]]\n",
    "            words_in_class[cl][1] = set([word.lower() for word in words_in_class[cl][1]])\n",
    "            count = {}\n",
    "            cls.log_likelihood[cl]={'bigram':{},'neg_word':{}}\n",
    "            for b in list(uniq_bigram.keys()):\n",
    "                count[b] = sorted_bigram[cl][b] if b in sorted_bigram[cl] else 0\n",
    "                prefix = b[0]\n",
    "                vc_class_doc = words_in_class[cl][0].count(prefix) + k*len(cls.V)\n",
    "                cls.log_likelihood[cl]['bigram'][b] = math.log((count[b]+k)/(vc_class_doc))\n",
    "            count = {}\n",
    "            for word in neg_vocab:\n",
    "                count[word] = words_in_class[cl][0].count(word)\n",
    "                vc_class_doc = len(words_in_class[cl][0]) + k*len(cls.V)\n",
    "                cls.log_likelihood[cl]['neg_word'][word] = math.log((count[word]+k)/(vc_class_doc))\n",
    "        return cls()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def count_for_class(data,class_name):\n",
    "    '''\n",
    "    This function returns number of docs(tweets) with respect\n",
    "    to a given input class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : List of docs with class\n",
    "        DESCRIPTION.\n",
    "    class_name : string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    count_of_class : int \n",
    "\n",
    "    '''\n",
    "    count_of_class = 0\n",
    "    for datum in data:\n",
    "        if(datum[1] == class_name):\n",
    "            count_of_class+=1\n",
    "    return count_of_class\n",
    "\n",
    "\n",
    "\n",
    "def vocab_for_class(data,class_name):\n",
    "    '''\n",
    "    This function creates vocabulary for a particular class and returns\n",
    "    a list of words in a class and set of unique words in that class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : List of docs with class\n",
    "        DESCRIPTION.\n",
    "    class_name : string.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        \n",
    "\n",
    "    '''\n",
    "    vocab = []\n",
    "    for word in data:\n",
    "        if(word[1] == class_name):\n",
    "            vocab+=word[0]\n",
    "    return [vocab,set(vocab)]\n",
    "\n",
    "\n",
    "def vocab(data):\n",
    "    '''\n",
    "    This function creates vocabulary set for a given Document\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list of documents with labelled classes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    set of unique words in set of Documents\n",
    "\n",
    "    '''\n",
    "    vocab = []\n",
    "    for word in data:\n",
    "        vocab+=word[0]\n",
    "    return set(vocab)\n",
    "\n",
    "def remove_unknown(train_vocab,test_vocab):\n",
    "    '''\n",
    "    It removes the words which are present in testing set but not\n",
    "    in training set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_vocab : vocabulary of training set\n",
    "    test_vocab : vocabulary of testing set\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_vocab : modified testing vocabulary\n",
    "\n",
    "    '''\n",
    "    x = set(test_vocab)\n",
    "    x = set(train_vocab).intersection(x)\n",
    "    for word in test_vocab:\n",
    "        if(word not in x):\n",
    "            test_vocab = list(filter(lambda w: w != word, test_vocab))\n",
    "    return test_vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_vocab = ['died','aggressor','terrifying','Isis','hunting','Savage','Islamofascim','non-believers','genocidal','antisemitism'\n",
    "            ,'communists','Insane','pedo','bitchy','harassed','dying','Islamofascist','wars','Destruction','smackem','fucktards',\n",
    "            'monsters','gross','crucify', 'over-sensitive','Crucifixion','psychopaths','feminist','feminazi','HATE','Crimes','mocks',\n",
    "            'ignorant','murders','hitting','rapists','#IDontNeedFeminism','frauds','punch','hoes','snobby','steal','stripper','kickass',\n",
    "            'brutal','harassment','execute','Insult','sewers','behead', 'damn','#FuckOff','raped','#Islamists','asshole',  'rats',\n",
    "             'violence','phony','chick', 'danger', 'stole', 'clowns', '#feminism', 'fools', 'Gags','Nazism','jihadis','atheism', 'morons',\n",
    "            'filthy', 'rant','inferior', 'Pigs', 'annoying', 'burnt','monger','faggot','#Notsexist','STFU','#sexism','FUCKING','cocks',\n",
    "           'grossly','tits', 'gruesome','butchers', 'terrorism', 'Trashy','hateful', 'useless', 'semen', 'dumbass', 'garbage','Hatemongers',\n",
    "            'punching','kills','cum','Satan','boobs','sexual','Terrorists','femininity','Jihad','#Bitches', 'gays','hater','arrogant',\n",
    "            'egomaniac','fat','fatty', 'hatered','Nazi','trashy','rapist', 'fuckin','fascist', 'Idiots','jihadists','grabbing', 'stalk', 'HOT',\n",
    "           'raping', 'faggots','dickless','murderer','sucking','fucker','horrible', 'shameful','fuck','Stalked','sassy', 'mantears', 'Islamofasicsm',\n",
    "             'maniacs','Sewer','Feminazi','nude','weaker','fucking','swine', 'shits','arse','BITCHES', 'Bitch', 'dickweed', 'pedophelia',\n",
    "            'cocksucker','fetish','sucks','chicks','DUMB','nigger','#YouSuck','SHUTUP','shiting','horny', 'nigga','dick','slaves','Islamolunatic',\n",
    "            '#IslamLOVESWomen','killer','butts','Hoes','crap', 'penis','hating','vaginas','stab','hatred','pedophile','bigotry'\n",
    "            ,'barbarity','scum', '#killerblondes']\n",
    "neg_vocab = set([word.lower() for word in neg_vocab])\n",
    "\n",
    "\n",
    "stop_words = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\"]\n",
    "def remove_stop_words(wordlist):\n",
    "    '''\n",
    "    It removes stop words like the,is,could,was,....\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    wordlist : list of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocab : modified list of words\n",
    "\n",
    "    '''\n",
    "    vocab = [word for word in wordlist if word not in stop_words]\n",
    "    return vocab\n",
    "\n",
    "\n",
    "\n",
    "def bigrams(data,cl):\n",
    "    '''\n",
    "    This function creates bigram for with respect to their class.\n",
    "    It returns a dictionary which contain bigram list for all the classes in data set\n",
    "    e.g in our case {\"offensive\":[],\"nonoffensive\"[]}\n",
    "    '''\n",
    "    \n",
    "    bigrams_in_corpus={}\n",
    "    for c in cl:\n",
    "        bigrams_in_corpus[c] = []\n",
    "        for sent in data:\n",
    "            if(sent[1]==c):\n",
    "                sent[0].insert(0,\"<s>\")\n",
    "                sent[0].insert(len(sent[0]),\"</s>\")\n",
    "                bigrams_in_corpus[c]+= [(sent[0][i].lower(),sent[0][i+1].lower()) for i in range(len(sent[0])-1)]\n",
    "        \n",
    "    return bigrams_in_corpus\n",
    "\n",
    "def bigrams_info(bigrams,cl):\n",
    "    '''\n",
    "    This function calculate some information about bigram. \n",
    "    1. It creates unique set of bigrams for each class, here  uniq_bi\n",
    "    2. It combines all unique bigrams into one variabe, uniq\n",
    "    3. It computes frequency of each bigram in its respective class, bi_freq\n",
    "    4. It combines frequency list from each class into one variable, uniq_freq\n",
    "    and finally returns uniq_freq and sorted_freq\n",
    "    \n",
    "    '''\n",
    "    uniq = []\n",
    "    uniq_bi = {}\n",
    "    bi_freq={}\n",
    "    uniq_freq={}\n",
    "    sorted_freq={}\n",
    "    for c in cl:\n",
    "        uniq_bi[c] = list(set(bigrams[c]))\n",
    "        uniq+=uniq_bi[c]\n",
    "        bi_freq[c] = {bi : bigrams[c].count(bi) for bi in uniq_bi[c]}\n",
    "        sorted_freq[c] = {word : value for word,value in sorted(bi_freq[c].items(),key=lambda item : item[1],reverse=True)}\n",
    "    \n",
    "    uniq = list(set(uniq))\n",
    "    uniq_freq = {bi:0 for bi in uniq}\n",
    "    for c in cl:\n",
    "        for bi,value in sorted_freq[c].items():\n",
    "            uniq_freq[bi]+=value\n",
    "    return (uniq_freq,sorted_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv,math,string\n",
    "import json,operator,random  \n",
    "def accuracy(classifier, data):\n",
    "    \"\"\"Computes the accuracy of a classifier on reference data.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        classifier: A classifier.\n",
    "        data: Reference data.\n",
    "\n",
    "    Returns:\n",
    "        The accuracy of the classifier on the test data, a float.\n",
    "    \"\"\"\n",
    "    \n",
    "    for d in data:\n",
    "        tweet = d[0]\n",
    "        actual = d[1]\n",
    "        predicted = classifier.predict(tweet)\n",
    "        if ( predicted is None):\n",
    "            continue \n",
    "           \n",
    "        # Some document(tweets) might be empty after removing unknown and stop words, in that case predict() returns None\n",
    "        # and continue to next document(tweet)\n",
    "        \n",
    "        if(actual == predicted):\n",
    "            if(predicted == 'offensive'):\n",
    "                classifier.tp+=1\n",
    "            else:\n",
    "                classifier.tn+=1\n",
    "        else:\n",
    "            if(predicted == 'offensive'):\n",
    "                classifier.fp+=1\n",
    "            else:\n",
    "                classifier.fn+=1\n",
    "    return (classifier.tp+classifier.tn)/(classifier.tp+classifier.tn+classifier.fn+classifier.fp)\n",
    "\n",
    "def f_1(classifier, data):\n",
    "    \"\"\"\n",
    "     Computes the F_1-score of a classifier on reference data.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        classifier: A classifier.\n",
    "        data: Reference data.\n",
    "\n",
    "    Returns:\n",
    "        The F_1-score of the classifier on the test data, a float.\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "   \n",
    "    precision = classifier.tp/(classifier.tp+classifier.fp)\n",
    "    recall  = classifier.tp/(classifier.tp+classifier.fn)\n",
    "    f1 = (2*precision*recall)/(precision+recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8018461538461539\n",
      "F_1:  0.30303030303030304\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes.train(train_data,0.1)\n",
    "print(\"Accuracy: \",accuracy(nb, test_data))\n",
    "print(\"F_1: \", f_1(nb,test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
