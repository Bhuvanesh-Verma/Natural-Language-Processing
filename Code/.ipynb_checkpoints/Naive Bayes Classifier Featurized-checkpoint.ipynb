{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 12896\n",
      "Test data: 3250\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def read_hate_tweets (annofile, jsonfile):\n",
    "    \"\"\"Reads in hate speech data.\"\"\"\n",
    "    all_data = {}\n",
    "    annos = {}\n",
    "    with open(annofile) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            if row[0] in annos:\n",
    "                # if duplicate with different rating, remove!\n",
    "                if row[1] != annos[row[0]]:\n",
    "                    del(annos[row[0]])\n",
    "            else:\n",
    "                annos[row[0]] = row[1]\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "                \n",
    "    with open(jsonfile) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            twtjson = json.loads(line)\n",
    "            twt_id = twtjson['id_str']\n",
    "            if twt_id in annos:\n",
    "                all_data[twt_id] = {}\n",
    "                all_data[twt_id]['offensive'] = \"nonoffensive\" if annos[twt_id] == 'none' else \"offensive\"\n",
    "                all_data[twt_id]['text_tok'] = tknzr.tokenize(twtjson['text'])\n",
    "\n",
    "    # split training and test data:\n",
    "    all_data_sorted = sorted(all_data.items())\n",
    "    items = [(i[1]['text_tok'],i[1]['offensive']) for i in all_data_sorted]\n",
    "    splititem = len(all_data)-3250\n",
    "    train_dt = items[:splititem]\n",
    "    test_dt = items[splititem:]\n",
    "    print('Training data:',len(train_dt))\n",
    "    print('Test data:',len(test_dt))\n",
    "\n",
    "    return(train_dt,test_dt)\n",
    "\n",
    "TWEETS_ANNO = '../Data/NAACL_SRW_2016.csv'\n",
    "TWEETS_TEXT = '../Data/NAACL_SRW_2016_tweets.json'\n",
    "\n",
    "(train_data,test_data) = read_hate_tweets(TWEETS_ANNO,TWEETS_TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    C=[]\n",
    "    V=[]\n",
    "    log_prior = {}\n",
    "    log_likelihood = {}\n",
    "    Bi = {}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tp = 0\n",
    "        self.tn = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        \"\"\"Predicts the class for a document.\n",
    "\n",
    "        Args:\n",
    "            x: A document, represented as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class, represented as a string.\n",
    "        \"\"\"\n",
    "        sum_ = {}\n",
    "        x = remove_unknown(self.V,x)\n",
    "        x = [word.lower() for word in x]\n",
    "        y=x \n",
    "        x.insert(0,\"<s>\")\n",
    "        x.insert(len(x),\"</s>\")\n",
    "        bigrams_in_x = [(x[i],x[i+1]) for i in range(len(x)-1)]\n",
    "\n",
    "        for cl in self.C:\n",
    "            tot = self.log_prior[cl]\n",
    "            for b in bigrams_in_x:\n",
    "                if b in list(self.Bi.keys()) :\n",
    "                    tot= tot + self.log_likelihood[cl]['bigram'][b]\n",
    "            for word in y:   \n",
    "                if word in neg_vocab:\n",
    "                    tot= tot + self.log_likelihood[cl]['neg_word'][word]   \n",
    "                    \n",
    "            sum_[cl] = tot\n",
    "            tot = 0\n",
    "        \n",
    "        if(len(sum_)>0):\n",
    "            return max(sum_.items(), key=operator.itemgetter(1))[0]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def train(cls, data, k=1):\n",
    "        \"\"\"Train a new classifier on training data using maximum\n",
    "        likelihood estimation and additive smoothing.\n",
    "\n",
    "        Args:\n",
    "            cls: The Python class representing the classifier.\n",
    "            data: Training data.\n",
    "            k: The smoothing constant.\n",
    "\n",
    "        Returns:\n",
    "            A trained classifier, an instance of `cls`.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_of_docs = len(data) # total number of tweets in our case\n",
    "        cls.C = set([word[1] for word in data]) # set of classes\n",
    "        cls.V = vocab(data) # set of unique words in training data\n",
    "        cls.V = remove_stop_words(cls.V)\n",
    "        cls.V = set([word.lower() for word in cls.V])\n",
    "        words_in_class = {}\n",
    "        bigram = bigrams(data,list(cls.C))\n",
    "        (uniq_bigram,sorted_bigram) = bigrams_info(bigram,cls.C)\n",
    "        cls.Bi = uniq_bigram\n",
    "        for cl in cls.C:\n",
    "            num_of_docs_in_c = count_for_class(data,cl)\n",
    "            cls.log_prior[cl] = math.log(num_of_docs_in_c/num_of_docs)\n",
    "            words_in_class[cl] = (vocab_for_class(data,cl))\n",
    "            words_in_class[cl][0] = remove_stop_words(words_in_class[cl][0])\n",
    "            words_in_class[cl][1] = remove_stop_words(list(words_in_class[cl][1]))\n",
    "            words_in_class[cl][0] = [word.lower() for word in words_in_class[cl][0]]\n",
    "            words_in_class[cl][1] = set([word.lower() for word in words_in_class[cl][1]])\n",
    "            count = {}\n",
    "            cls.log_likelihood[cl]={'bigram':{},'neg_word':{}}\n",
    "            for b in list(uniq_bigram.keys()):\n",
    "                count[b] = sorted_bigram[cl][b] if b in sorted_bigram[cl] else 0\n",
    "                prefix = b[0]\n",
    "                vc_class_doc = words_in_class[cl][0].count(prefix) + k*len(cls.V)\n",
    "                cls.log_likelihood[cl]['bigram'][b] = math.log((count[b]+k)/(vc_class_doc))\n",
    "            count = {}\n",
    "            for word in neg_vocab:\n",
    "                count[word] = words_in_class[cl][0].count(word)\n",
    "                vc_class_doc = len(words_in_class[cl][1]) + k*len(cls.V)\n",
    "                cls.log_likelihood[cl]['neg_word'][word] = math.log((count[word]+k)/(vc_class_doc))\n",
    "        return cls()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
