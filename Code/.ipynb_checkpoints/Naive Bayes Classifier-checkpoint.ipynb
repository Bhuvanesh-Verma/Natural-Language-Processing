{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes Classifier use Bayes Ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 12896\n",
      "Test data: 3250\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def read_hate_tweets (annofile, jsonfile):\n",
    "    \"\"\"Reads in hate speech data.\"\"\"\n",
    "    all_data = {}\n",
    "    annos = {}\n",
    "    with open(annofile) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            if row[0] in annos:\n",
    "                # if duplicate with different rating, remove!\n",
    "                if row[1] != annos[row[0]]:\n",
    "                    del(annos[row[0]])\n",
    "            else:\n",
    "                annos[row[0]] = row[1]\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "                \n",
    "    with open(jsonfile) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            twtjson = json.loads(line)\n",
    "            twt_id = twtjson['id_str']\n",
    "            if twt_id in annos:\n",
    "                all_data[twt_id] = {}\n",
    "                all_data[twt_id]['offensive'] = \"nonoffensive\" if annos[twt_id] == 'none' else \"offensive\"\n",
    "                all_data[twt_id]['text_tok'] = tknzr.tokenize(twtjson['text'])\n",
    "\n",
    "    # split training and test data:\n",
    "    all_data_sorted = sorted(all_data.items())\n",
    "    items = [(i[1]['text_tok'],i[1]['offensive']) for i in all_data_sorted]\n",
    "    splititem = len(all_data)-3250\n",
    "    train_dt = items[:splititem]\n",
    "    test_dt = items[splititem:]\n",
    "    print('Training data:',len(train_dt))\n",
    "    print('Test data:',len(test_dt))\n",
    "\n",
    "    return(train_dt,test_dt)\n",
    "\n",
    "TWEETS_ANNO = '../Data/NAACL_SRW_2016.csv'\n",
    "TWEETS_TEXT = '../Data/NAACL_SRW_2016_tweets.json'\n",
    "\n",
    "(train_data,test_data) = read_hate_tweets(TWEETS_ANNO,TWEETS_TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    C=[] # Classes : It is list of classes in dataset\n",
    "    V=[] # Vocabulary : It is list of unique tokens in dataset\n",
    "    log_prior = {} # It stores log_prior wrt to class, eg: {'class1':value,'class2':value,....}\n",
    "    log_likelihood = {} # It stores log_likelihood wrt to class, eg: {'class1':{word1:value,word2:value,..},'class2':{},....}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tp = 0 # True Positive \n",
    "        self.tn = 0 # True Negative\n",
    "        self.fp = 0 # False Positive\n",
    "        self.fn = 0 # False Negative\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        \"\"\"Predicts the class for a document.\n",
    "\n",
    "        Args:\n",
    "            x: A document, represented as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class, represented as a string.\n",
    "        \"\"\"\n",
    "        \n",
    "        sum_ = {} # Variable to store sum of log probability as { 'class1': value, 'class2': value , ....}\n",
    "        x = remove_unknown(self.V,x) # removes words which are not present in Vocabulary but in test document\n",
    "        x = [word.lower() for word in x] \n",
    "        for cl in self.C:\n",
    "            tot = self.log_prior[cl]\n",
    "            for word in x:\n",
    "                if word in self.V:\n",
    "                    tot=tot+self.log_likelihood[cl][word]\n",
    "            sum_[cl] = tot\n",
    "            tot=0\n",
    "        \n",
    "        if(len(sum_)>0):\n",
    "            return max(sum_.items(), key=operator.itemgetter(1))[0]\n",
    "        else:\n",
    "            return None #Returns None if remove_unkown() returns a empty list\n",
    "        \n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def train(cls, data, k=1):\n",
    "        \"\"\"Train a new classifier on training data using maximum\n",
    "        likelihood estimation and additive smoothing.\n",
    "\n",
    "        Args:\n",
    "            cls: The Python class representing the classifier.\n",
    "            data: Training data.\n",
    "            k: The smoothing constant.\n",
    "\n",
    "        Returns:\n",
    "            A trained classifier, an instance of `cls`.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_of_docs = len(data) # total number of tweets in our case\n",
    "        cls.C = set([word[1] for word in data]) # set of classes\n",
    "        cls.V = vocab(data) # set of unique words in training data\n",
    "        cls.V = remove_stop_words(cls.V)\n",
    "        cls.V = [word.lower() for word in cls.V]\n",
    "        words_in_class = {} # It is dictionary which contain tokens and vocabulary wrt class\n",
    "                            # eg: { 'class1' : [[list of token],[list of unique tokens or vocabulary]], 'class2':[[],[]],...}\n",
    "        for cl in cls.C:\n",
    "            num_of_docs_in_c = count_for_class(data,cl)\n",
    "            cls.log_prior[cl] = math.log(num_of_docs_in_c/num_of_docs)\n",
    "            words_in_class[cl] = (vocab_for_class(data,cl))\n",
    "            words_in_class[cl][0] = remove_stop_words(words_in_class[cl][0])\n",
    "            words_in_class[cl][1] = remove_stop_words(list(words_in_class[cl][1]))\n",
    "            words_in_class[cl][0] = [word.lower() for word in words_in_class[cl][0]]\n",
    "            words_in_class[cl][1] = [word.lower() for word in words_in_class[cl][1]]\n",
    "            count = {}\n",
    "            cls.log_likelihood[cl] = {}\n",
    "            for word in cls.V:\n",
    "                count[word] = words_in_class[cl][0].count(word)\n",
    "                vc_class_doc = len(words_in_class[cl][1]) + k*len(cls.V)\n",
    "                cls.log_likelihood[cl][word] = math.log((count[word]+k)/(vc_class_doc))\n",
    "        return cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
