{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 12896\n",
      "Test data: 3250\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def read_hate_tweets (annofile, jsonfile):\n",
    "    \"\"\"Reads in hate speech data.\"\"\"\n",
    "    all_data = {}\n",
    "    annos = {}\n",
    "    with open(annofile) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            if row[0] in annos:\n",
    "                # if duplicate with different rating, remove!\n",
    "                if row[1] != annos[row[0]]:\n",
    "                    del(annos[row[0]])\n",
    "            else:\n",
    "                annos[row[0]] = row[1]\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "                \n",
    "    with open(jsonfile) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            twtjson = json.loads(line)\n",
    "            twt_id = twtjson['id_str']\n",
    "            if twt_id in annos:\n",
    "                all_data[twt_id] = {}\n",
    "                all_data[twt_id]['offensive'] = \"nonoffensive\" if annos[twt_id] == 'none' else \"offensive\"\n",
    "                all_data[twt_id]['text_tok'] = tknzr.tokenize(twtjson['text'])\n",
    "\n",
    "    # split training and test data:\n",
    "    all_data_sorted = sorted(all_data.items())\n",
    "    items = [(i[1]['text_tok'],i[1]['offensive']) for i in all_data_sorted]\n",
    "    splititem = len(all_data)-3250\n",
    "    train_dt = items[:splititem]\n",
    "    test_dt = items[splititem:]\n",
    "    print('Training data:',len(train_dt))\n",
    "    print('Test data:',len(test_dt))\n",
    "\n",
    "    return(train_dt,test_dt)\n",
    "\n",
    "TWEETS_ANNO = '../Data/NAACL_SRW_2016.csv'\n",
    "TWEETS_TEXT = '../Data/NAACL_SRW_2016_tweets.json'\n",
    "\n",
    "(train_data,test_data) = read_hate_tweets(TWEETS_ANNO,TWEETS_TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_w2i(data):\n",
    "    '''\n",
    "    This function creates a vector of unique words in dataset excluding some stop words.\n",
    "    \n",
    "    '''\n",
    "    vocab = []\n",
    "    for word in data:\n",
    "        vocab+=word[0]\n",
    "    return set(vocab)\n",
    "def featurize(data):\n",
    "    \n",
    "    '''\n",
    "    This function creates a matrix (X) with rows representing each data instance and columns representing features.\n",
    "    Features in this case is Vocabulary set. It assigns value 1 if word is present in data instance (tweet) else\n",
    "    it assigns 0. \n",
    "    \n",
    "    It creates another matrix (Y) with rows representing number of data instances and column represented by number of classes.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    X = np.array([[0 for j in range(len(vocab))],]*len(data))\n",
    "    i=0\n",
    "    for obj in data:\n",
    "        tweet = [word.lower() for word in list(obj[0])]\n",
    "        j=0\n",
    "        for word in vocab:\n",
    "            if(word in tweet):\n",
    "                X[i][j] = 1\n",
    "            else:\n",
    "                X[i][j] = 0\n",
    "            j+=1\n",
    "        i+=1  \n",
    "    Y = np.array([[0,1],]*len(data))\n",
    "    for i in range(len(data)):\n",
    "        if(data[i][1]=='offensive'):\n",
    "            Y[i] = [1,0]\n",
    "    return (X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(build_w2i(train_data))\n",
    "(x,y) = featurize(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogReg:\n",
    "\n",
    "    num_class = 0\n",
    "    def __init__(self, eta=0.01, num_iter=30):\n",
    "        self.eta = eta\n",
    "        self.num_iter = num_iter\n",
    "        \n",
    "    def softmax(self,x):\n",
    "        '''\n",
    "        This function computes softmax value for an array or a matrix\n",
    "\n",
    "        Input : x is a array or a matrix\n",
    "\n",
    "        Output : result is either a array of softmax value or a matrix\n",
    "        '''\n",
    "        x=x.astype(float)\n",
    "        if x.ndim==1:\n",
    "            return np.exp(x)/np.sum(np.exp(x))\n",
    "        elif x.ndim==2:\n",
    "            result=np.zeros_like(x)\n",
    "            M,N=x.shape\n",
    "            z = x - np.max(x, axis=0, keepdims=True)\n",
    "            for n in range(N):\n",
    "                S=np.sum(np.exp(z[:,n]))\n",
    "                result[:,n]=np.exp(z[:,n])/S\n",
    "            return result\n",
    "        else:\n",
    "            print(\"The input array is not 1- or 2-dimensional.\")\n",
    "    \n",
    "    \n",
    "    def gradient(self,y_pred,Y,X):\n",
    "        \n",
    "        '''\n",
    "        This function computes gradient using calculated output (y_), actual output(y) and input matrix (x)\n",
    "        x is matrix which contains data instance wrt their weights.\n",
    "        \n",
    "        \"grad\" variable is a matrix with rows representing number of classes (2) and column representing \n",
    "         features(words in Vocabulary).\n",
    "        \n",
    "        It returns improved weights and bias for each features wrt their classes.\n",
    "        \n",
    "        '''\n",
    "        temp = np.subtract(y_pred,Y)\n",
    "        weight = np.matmul(temp,X)/len(X)\n",
    "        bias = np.matmul(temp,np.ones((X.shape)))/len(X)\n",
    "        return (weight,bias)\n",
    "    \n",
    "    def cost(self,y_pred,Y):\n",
    "        error = np.sum(-np.multiply(Y,np.log(y_pred,where= y_pred !=0)))\n",
    "        return error\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        This function trains given dataset by iteratively updating their weights and bias.\n",
    "        weights is matrix of size (num of classes X num of words in Vocabulary)\n",
    "        bias is matrix of size (num of classes X 1)\n",
    "        \n",
    "        We divide dataset into mini-batches of 100 data instances and update weights and bias at end of each mini batch.\n",
    "        We repeat this process num_iter times\n",
    "        \n",
    "        '''\n",
    "        # weights initialization\n",
    "        self.num_class = Y.shape[1]\n",
    "        self.weights = np.zeros((Y.shape[1],X.shape[1]))\n",
    "        self.bias = np.zeros((Y.shape[1],X.shape[1]))\n",
    "        for i in range(self.num_iter):\n",
    "            ind = np.arange(len(X))\n",
    "            np.random.shuffle(ind)\n",
    "            b=np.arange(0,len(X),100)\n",
    "            np.append(b,len(X))\n",
    "            loss = 0\n",
    "            j=0\n",
    "            for j in range(len(b)-1):\n",
    "                x = X[ind[b[j]:b[j+1]]]\n",
    "                y = Y[ind[b[j]:b[j+1]]]\n",
    "                prob = np.zeros((2,len(x)))\n",
    "                prob = self.p(x)\n",
    "                error = self.cost(prob,y.T)\n",
    "                loss = loss + error/len(x)\n",
    "                change_in_w, change_in_b = self.gradient(prob,y.T,x)\n",
    "                self.weights = self.weights - self.eta*change_in_w\n",
    "                self.bias = self.bias - self.eta*change_in_b\n",
    "                \n",
    "            print(\"Loss : \",loss)\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def p(self, X): \n",
    "        '''\n",
    "        This function cpmputes probability for each data instance wrt each class.\n",
    "        \n",
    "        \"prob\" is a matrix of size ( num of data instances X num of classes )\n",
    "        For our particular case first case represent class \"nonoffensive\" and second column represent \"offensive\"\n",
    "        i.e prob[i][0] is probability for y=0 and prob[i][1] is probability for y=1 where y is actual output and\n",
    "        offensive means 1.\n",
    "        \n",
    "        '''\n",
    "        temp = np.add(np.matmul(self.weights,X.T),np.matmul(self.bias,np.ones((X.T.shape)))/X.shape[1])\n",
    "        prob = np.zeros((self.weights.shape))\n",
    "        prob = self.softmax(temp) \n",
    "        return prob\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        This function predicts the class to which given data instance belongs to.\n",
    "        We compare the calculated probability of given instance among all classes.\n",
    "        Class with max probability is the output.\n",
    "        \n",
    "        It also calculates F_1 score.\n",
    "        '''\n",
    "        (x_test,y_test) = featurize(X)\n",
    "        prob=self.p(x_test)\n",
    "        result=np.argmax(prob,axis=0)\n",
    "        tp=0\n",
    "        tn=0\n",
    "        fn=0\n",
    "        fp=0\n",
    "        y=y_test.T\n",
    "        print(y.shape)\n",
    "        print(y_test.shape)\n",
    "        for i in range(len(y_test)):\n",
    "            if(y[0][i]==result[i]):\n",
    "                if(result[i]==1):\n",
    "                    tp+=1\n",
    "                else:\n",
    "                    tn+=1\n",
    "            else:\n",
    "                if(result[i]==1):\n",
    "                    fp+=1\n",
    "                else:\n",
    "                    fn+=1\n",
    "        if(tp+fp!=0):\n",
    "            pr = tp/(tp+fp)\n",
    "        if(tp+fn!=0):\n",
    "            r = tp/(tp+fn)\n",
    "        if(pr+r!=0):\n",
    "            f1 = 2*pr*r/(pr+r)\n",
    "        else:\n",
    "            f1=0\n",
    "        #acc = (tp+tn)/(tp+tn+fn+fp)\n",
    "        print(\"F_1 score is: \"+str(f1))\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  75.49172826982984\n",
      "Loss :  66.82921192987259\n",
      "Loss :  62.85509751221519\n",
      "Loss :  60.30692195601471\n",
      "Loss :  58.42204684456027\n",
      "Loss :  56.894831460004376\n",
      "Loss :  55.777390703681306\n",
      "Loss :  54.597680608866014\n",
      "Loss :  53.73527502741612\n",
      "Loss :  52.96797730627935\n",
      "Loss :  52.158206120374516\n",
      "Loss :  51.55070595302376\n",
      "Loss :  50.90302001996976\n",
      "Loss :  50.35064738803884\n",
      "Loss :  49.87339678106835\n",
      "Loss :  49.34264864902467\n",
      "Loss :  48.88089718262399\n",
      "Loss :  48.564671761921836\n",
      "Loss :  48.12753711932712\n",
      "Loss :  47.727955881343036\n",
      "Loss :  47.37384158981557\n",
      "Loss :  47.02218262473904\n",
      "Loss :  46.652321103569705\n",
      "Loss :  46.463455715470495\n",
      "Loss :  46.12508138279073\n",
      "Loss :  45.832387306386416\n",
      "Loss :  45.45410039450242\n",
      "Loss :  45.264218784114505\n",
      "Loss :  45.04996983120211\n",
      "Loss :  44.740879514888015\n",
      "Loss :  44.544603674867744\n",
      "Loss :  44.282984507927395\n",
      "Loss :  43.99017382831343\n",
      "Loss :  43.84317535067141\n",
      "Loss :  43.60197370043039\n",
      "Loss :  43.43148872374808\n",
      "Loss :  43.223792087079175\n",
      "Loss :  42.9854179036825\n",
      "Loss :  42.75436341477278\n",
      "Loss :  42.6078169018975\n",
      "Loss :  42.3787627541323\n",
      "Loss :  42.225805079898706\n",
      "Loss :  42.02066710623007\n",
      "Loss :  41.90819644880254\n",
      "Loss :  41.623771706946506\n",
      "Loss :  41.570988659441305\n",
      "Loss :  41.38508929042676\n",
      "Loss :  41.15438644103442\n",
      "Loss :  41.060634004035315\n",
      "Loss :  40.88472798143586\n",
      "Loss :  40.73368007146176\n",
      "Loss :  40.65097132469707\n",
      "Loss :  40.48377513293713\n",
      "Loss :  40.306952487871804\n",
      "Loss :  40.19608862558932\n",
      "Loss :  39.99819326146577\n",
      "Loss :  39.89186850803938\n",
      "Loss :  39.68755541568604\n",
      "Loss :  39.61179150099853\n",
      "Loss :  39.51113590117387\n",
      "Loss :  39.37701916793491\n",
      "Loss :  39.221316517668114\n",
      "Loss :  39.13134871690399\n",
      "Loss :  38.973359747188134\n",
      "Loss :  38.76715065498152\n",
      "Loss :  38.641470288128104\n",
      "Loss :  38.623821173874\n",
      "Loss :  38.4207776192632\n",
      "Loss :  38.387595172736376\n",
      "Loss :  38.28204963385245\n",
      "(2, 3250)\n",
      "(3250, 2)\n",
      "F_1 score is: 0.1737534414193943\n"
     ]
    }
   ],
   "source": [
    "lg=LogReg(0.1,70)\n",
    "lg.train(x,y)\n",
    "lg.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
