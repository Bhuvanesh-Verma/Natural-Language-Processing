{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 12896\n",
      "Test data: 3250\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def read_hate_tweets (annofile, jsonfile):\n",
    "    \"\"\"Reads in hate speech data.\"\"\"\n",
    "    all_data = {}\n",
    "    annos = {}\n",
    "    with open(annofile) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            if row[0] in annos:\n",
    "                # if duplicate with different rating, remove!\n",
    "                if row[1] != annos[row[0]]:\n",
    "                    del(annos[row[0]])\n",
    "            else:\n",
    "                annos[row[0]] = row[1]\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "                \n",
    "    with open(jsonfile) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            twtjson = json.loads(line)\n",
    "            twt_id = twtjson['id_str']\n",
    "            if twt_id in annos:\n",
    "                all_data[twt_id] = {}\n",
    "                all_data[twt_id]['offensive'] = \"nonoffensive\" if annos[twt_id] == 'none' else \"offensive\"\n",
    "                all_data[twt_id]['text_tok'] = tknzr.tokenize(twtjson['text'])\n",
    "\n",
    "    # split training and test data:\n",
    "    all_data_sorted = sorted(all_data.items())\n",
    "    items = [(i[1]['text_tok'],i[1]['offensive']) for i in all_data_sorted]\n",
    "    splititem = len(all_data)-3250\n",
    "    train_dt = items[:splititem]\n",
    "    test_dt = items[splititem:]\n",
    "    print('Training data:',len(train_dt))\n",
    "    print('Test data:',len(test_dt))\n",
    "\n",
    "    return(train_dt,test_dt)\n",
    "\n",
    "TWEETS_ANNO = '../Data/NAACL_SRW_2016.csv'\n",
    "TWEETS_TEXT = '../Data/NAACL_SRW_2016_tweets.json'\n",
    "\n",
    "(train_data,test_data) = read_hate_tweets(TWEETS_ANNO,TWEETS_TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_w2i(data):\n",
    "    '''\n",
    "    This function creates a vector of unique words in dataset excluding some stop words.\n",
    "    \n",
    "    '''\n",
    "    vocab = []\n",
    "    for word in data:\n",
    "        vocab+=word[0]\n",
    "    return set(vocab)\n",
    "def featurize(data):\n",
    "    \n",
    "    '''\n",
    "    This function creates a matrix (X) with rows representing each data instance and columns representing features.\n",
    "    Features in this case is Vocabulary set. It assigns value 1 if word is present in data instance (tweet) else\n",
    "    it assigns 0. \n",
    "    \n",
    "    It creates another matrix (Y) with rows representing number of data instances and column represented by number of classes.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    X = np.array([[0 for j in range(len(vocab))],]*len(data))\n",
    "    i=0\n",
    "    for obj in data:\n",
    "        tweet = [word.lower() for word in list(obj[0])]\n",
    "        j=0\n",
    "        for word in vocab:\n",
    "            if(word in tweet):\n",
    "                X[i][j] = 1\n",
    "            else:\n",
    "                X[i][j] = 0\n",
    "            j+=1\n",
    "        i+=1  \n",
    "    Y = np.array([[0,1],]*len(data))\n",
    "    for i in range(len(data)):\n",
    "        if(data[i][1]=='offensive'):\n",
    "            Y[i] = [1,0]\n",
    "    return (X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(build_w2i(train_data))\n",
    "(x,y) = featurize(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12896, 25892) (12896, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogReg:\n",
    "\n",
    "    num_class = 0\n",
    "    def __init__(self, eta=0.01, num_iter=30 , alpha = 0.1):\n",
    "        self.eta = eta\n",
    "        self.num_iter = num_iter\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def softmax(self,x):\n",
    "        '''\n",
    "        This function computes softmax value for an array or a matrix\n",
    "\n",
    "        Input : x is a array or a matrix\n",
    "\n",
    "        Output : result is either a array of softmax value or a matrix\n",
    "        '''\n",
    "        x=x.astype(float)\n",
    "        if x.ndim==1:\n",
    "            return np.exp(x)/np.sum(np.exp(x))\n",
    "        elif x.ndim==2:\n",
    "            result=np.zeros_like(x)\n",
    "            M,N=x.shape\n",
    "            z = x - np.max(x, axis=0, keepdims=True)\n",
    "            for n in range(N):\n",
    "                S=np.sum(np.exp(z[:,n]))\n",
    "                result[:,n]=np.exp(z[:,n])/S\n",
    "            return result\n",
    "        else:\n",
    "            print(\"The input array is not 1- or 2-dimensional.\")\n",
    "    \n",
    "    \n",
    "    def gradient(self,y_pred,Y,X):\n",
    "        \n",
    "        '''\n",
    "        This function computes gradient using calculated output (y_), actual output(y) and input matrix (x)\n",
    "        x is matrix which contains data instance wrt their weights.\n",
    "        \n",
    "        \n",
    "        It returns improved weights and bias for each features wrt their classes.\n",
    "        \n",
    "        '''\n",
    "        temp = np.subtract(y_pred,Y)\n",
    "        weight = np.matmul(temp,X)\n",
    "        bias = np.matmul(temp,np.ones((X.shape)))\n",
    "        return (weight,bias)\n",
    "    \n",
    "    def cost(self,y_pred,Y):\n",
    "        error = np.sum(-np.multiply(Y,np.log(y_pred,where= y_pred !=0)))\n",
    "        return error\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        This function trains given dataset by iteratively updating their weights and bias.\n",
    "        weights is matrix of size (num of classes X num of words in Vocabulary)\n",
    "        bias is matrix of size (num of classes X 1)\n",
    "        \n",
    "        We divide dataset into mini-batches of 100 data instances and update weights and bias at end of each mini batch.\n",
    "        We repeat this process num_iter times\n",
    "        \n",
    "        '''\n",
    "        # weights initialization\n",
    "        self.num_class = Y.shape[1]\n",
    "        self.weights = np.zeros((Y.shape[1],X.shape[1]))\n",
    "        self.bias = np.zeros((Y.shape[1],X.shape[1]))\n",
    "        for i in range(self.num_iter):\n",
    "            ind = np.arange(len(X))\n",
    "            np.random.shuffle(ind)\n",
    "            b=np.arange(0,len(X),100)\n",
    "            np.append(b,len(X))\n",
    "            loss = 0\n",
    "            j=0\n",
    "            for j in range(len(b)-1):\n",
    "                x = X[ind[b[j]:b[j+1]]]\n",
    "                y = Y[ind[b[j]:b[j+1]]]\n",
    "                prob = np.zeros((2,len(x)))\n",
    "                prob = self.p(x)\n",
    "                error = self.cost(prob,y.T)\n",
    "                loss = loss + error/len(x)\n",
    "                change_in_w, change_in_b = self.gradient(prob,y.T,x)\n",
    "                #change_in_w = change_in_w/len(b)\n",
    "                change_in_b = change_in_b/len(b)\n",
    "                self.weights = self.weights - self.eta*(change_in_w-\n",
    "                                                        self.alpha*np.square(self.weights - np.max(self.weights, axis=0, keepdims=True)))\n",
    "                self.bias = self.bias - self.eta*(change_in_b-\n",
    "                                                  self.alpha*np.square(self.bias - np.max(self.bias, axis=0, keepdims=True)))\n",
    "                \n",
    "            print(\"Loss : \",loss)\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def p(self, X): \n",
    "        '''\n",
    "        This function cpmputes probability for each data instance wrt each class.\n",
    "        \n",
    "        '''\n",
    "        temp = np.add(np.matmul(self.weights,X.T),np.matmul(self.bias,np.ones((X.T.shape)))/X.shape[1])\n",
    "        prob = self.softmax(temp) \n",
    "        return prob\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        This function predicts the class to which given data instance belongs to.\n",
    "        We compare the calculated probability of given instance among all classes.\n",
    "        Class with max probability is the output.\n",
    "        \n",
    "        It also calculates F_1 score.\n",
    "        '''\n",
    "        (x_test,y_test) = featurize(X)\n",
    "        prob=self.p(x_test)\n",
    "        result=np.argmax(prob,axis=0)\n",
    "        tp=0\n",
    "        tn=0\n",
    "        fn=0\n",
    "        fp=0\n",
    "        y=y_test.T\n",
    "        print(result)\n",
    "        print(y_test.shape)\n",
    "        for i in range(len(y_test)):\n",
    "            if(y[0][i]!=result[i]):\n",
    "                if(result[i]==0):\n",
    "                    tp+=1\n",
    "                else:\n",
    "                    tn+=1\n",
    "            else:\n",
    "                if(result[i]==0):\n",
    "                    fp+=1\n",
    "                else:\n",
    "                    fn+=1\n",
    "        if(tp+fp!=0):\n",
    "            pr = tp/(tp+fp)\n",
    "        if(tp+fn!=0):\n",
    "            r = tp/(tp+fn)\n",
    "        if(pr+r!=0):\n",
    "            f1 = 2*pr*r/(pr+r)\n",
    "        else:\n",
    "            f1=0\n",
    "        acc = (tp+tn)/(tp+tn+fn+fp)\n",
    "        print(\"F_1 score is: \",f1)\n",
    "        print('Accuracy : ',acc)\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  65.53085603966362\n",
      "Loss :  55.027926569464036\n",
      "Loss :  50.806736504646906\n",
      "Loss :  47.88826273968885\n",
      "Loss :  45.613139790623244\n",
      "Loss :  43.79848537902119\n",
      "Loss :  42.24806780312386\n",
      "Loss :  40.875265538162125\n",
      "Loss :  39.59293366225303\n",
      "Loss :  38.59692872093079\n",
      "Loss :  37.64610807106132\n",
      "Loss :  36.75380627732457\n",
      "Loss :  35.891603796436065\n",
      "Loss :  35.24308269285592\n",
      "Loss :  34.41081762647342\n",
      "Loss :  33.711297846487064\n",
      "Loss :  33.114410161474524\n",
      "Loss :  32.62912841526994\n",
      "Loss :  32.14910273364636\n",
      "Loss :  31.653977789983184\n",
      "Loss :  31.112044802944244\n",
      "Loss :  30.55664039602641\n",
      "Loss :  30.126394200206782\n",
      "Loss :  29.736913833353178\n",
      "Loss :  29.31454069867564\n",
      "Loss :  29.017656499239067\n",
      "Loss :  28.55701329309474\n",
      "Loss :  28.17484257554147\n",
      "Loss :  27.768921871796742\n",
      "Loss :  27.480284321692736\n",
      "[1 1 1 ... 1 1 0]\n",
      "(3250, 2)\n",
      "F_1 score is:  0.5541195476575121\n",
      "Accuracy :  0.8301538461538461\n"
     ]
    }
   ],
   "source": [
    "lg=LogReg(0.01,num_iter=30)\n",
    "lg.train(x,y)\n",
    "lg.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  65.70102433534275\n",
      "Loss :  55.461229334512694\n",
      "Loss :  51.44207179877521\n",
      "Loss :  48.68328120674818\n",
      "Loss :  46.69114345609151\n",
      "Loss :  45.14492824450215\n",
      "Loss :  44.07408471525496\n",
      "Loss :  43.08937074834608\n",
      "Loss :  42.120037123002405\n",
      "Loss :  41.36572991573463\n",
      "Loss :  40.73139790778097\n",
      "Loss :  40.177579141927346\n",
      "Loss :  39.83388653715795\n",
      "Loss :  39.3436698737661\n",
      "Loss :  38.979776546764214\n",
      "Loss :  38.56366541295866\n",
      "Loss :  38.26847329120319\n",
      "Loss :  38.0301649319871\n",
      "Loss :  37.80359209607566\n",
      "Loss :  37.487245607885974\n",
      "Loss :  37.36177972986984\n",
      "Loss :  37.16145238260284\n",
      "Loss :  36.991502353890475\n",
      "Loss :  36.81996334526019\n",
      "Loss :  36.64317005422535\n",
      "Loss :  36.55198824161171\n",
      "Loss :  36.45194666292349\n",
      "Loss :  36.37616381047975\n",
      "Loss :  36.23727884664787\n",
      "Loss :  36.20353963223829\n"
     ]
    }
   ],
   "source": [
    "lg=LogReg(0.01,num_iter=30,alpha=0.1)\n",
    "lg.train(x,y)\n",
    "lg.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
