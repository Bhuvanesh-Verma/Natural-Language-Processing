{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 12896\n",
      "Test data: 3250\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def read_hate_tweets (annofile, jsonfile):\n",
    "    \"\"\"Reads in hate speech data.\"\"\"\n",
    "    all_data = {}\n",
    "    annos = {}\n",
    "    with open(annofile) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            if row[0] in annos:\n",
    "                # if duplicate with different rating, remove!\n",
    "                if row[1] != annos[row[0]]:\n",
    "                    del(annos[row[0]])\n",
    "            else:\n",
    "                annos[row[0]] = row[1]\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "                \n",
    "    with open(jsonfile) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            twtjson = json.loads(line)\n",
    "            twt_id = twtjson['id_str']\n",
    "            if twt_id in annos:\n",
    "                all_data[twt_id] = {}\n",
    "                all_data[twt_id]['offensive'] = \"nonoffensive\" if annos[twt_id] == 'none' else \"offensive\"\n",
    "                all_data[twt_id]['text_tok'] = tknzr.tokenize(twtjson['text'])\n",
    "\n",
    "    # split training and test data:\n",
    "    all_data_sorted = sorted(all_data.items())\n",
    "    items = [(i[1]['text_tok'],i[1]['offensive']) for i in all_data_sorted]\n",
    "    splititem = len(all_data)-3250\n",
    "    train_dt = items[:splititem]\n",
    "    test_dt = items[splititem:]\n",
    "    print('Training data:',len(train_dt))\n",
    "    print('Test data:',len(test_dt))\n",
    "\n",
    "    return(train_dt,test_dt)\n",
    "\n",
    "TWEETS_ANNO = '../Data/NAACL_SRW_2016.csv'\n",
    "TWEETS_TEXT = '../Data/NAACL_SRW_2016_tweets.json'\n",
    "\n",
    "(train_data,test_data) = read_hate_tweets(TWEETS_ANNO,TWEETS_TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_w2i(data):\n",
    "    '''\n",
    "    This function creates a vector of unique words in dataset excluding some stop words.\n",
    "    \n",
    "    '''\n",
    "    vocab = []\n",
    "    for word in data:\n",
    "        vocab+=word[0]\n",
    "    return set(vocab)\n",
    "def featurize(data):\n",
    "    \n",
    "    '''\n",
    "    This function creates a matrix (X) with rows representing each data instance and columns representing features.\n",
    "    Features in this case is Vocabulary set. It assigns value 1 if word is present in data instance (tweet) else\n",
    "    it assigns 0. \n",
    "    \n",
    "    It creates another matrix (Y) with rows representing number of data instances and column represented by number of classes.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    X = np.array([[0 for j in range(len(vocab))],]*len(data))\n",
    "    i=0\n",
    "    for obj in data:\n",
    "        tweet = [word.lower() for word in list(obj[0])]\n",
    "        j=0\n",
    "        for word in vocab:\n",
    "            if(word in tweet):\n",
    "                X[i][j] = 1\n",
    "            else:\n",
    "                X[i][j] = 0\n",
    "            j+=1\n",
    "        i+=1  \n",
    "    Y = np.array([[0,1],]*len(data))\n",
    "    for i in range(len(data)):\n",
    "        if(data[i][1]=='offensive'):\n",
    "            Y[i] = [1,0]\n",
    "    return (X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(build_w2i(train_data))\n",
    "(x,y) = featurize(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogReg:\n",
    "\n",
    "    num_class = 0\n",
    "    def __init__(self, eta=0.01, num_iter=30):\n",
    "        self.eta = eta\n",
    "        self.num_iter = num_iter\n",
    "        \n",
    "    def softmax(self,x):\n",
    "        '''\n",
    "        This function computes softmax value for an array or a matrix\n",
    "\n",
    "        Input : x is a array or a matrix\n",
    "\n",
    "        Output : result is either a array of softmax value or a matrix\n",
    "        '''\n",
    "        x=x.astype(float)\n",
    "        if x.ndim==1:\n",
    "            return np.exp(x)/np.sum(np.exp(x))\n",
    "        elif x.ndim==2:\n",
    "            result=np.zeros_like(x)\n",
    "            M,N=x.shape\n",
    "            z = x - np.max(x, axis=0, keepdims=True)\n",
    "            for n in range(N):\n",
    "                S=np.sum(np.exp(z[:,n]))\n",
    "                result[:,n]=np.exp(z[:,n])/S\n",
    "            return result\n",
    "        else:\n",
    "            print(\"The input array is not 1- or 2-dimensional.\")\n",
    "    \n",
    "    \n",
    "    def gradient(self,y_pred,Y,X):\n",
    "        \n",
    "        '''\n",
    "        This function computes gradient using calculated output (y_), actual output(y) and input matrix (x)\n",
    "        x is matrix which contains data instance wrt their weights.\n",
    "        \n",
    "        \"grad\" variable is a matrix with rows representing number of classes (2) and column representing \n",
    "         features(words in Vocabulary).\n",
    "        \n",
    "        It returns improved weights and bias for each features wrt their classes.\n",
    "        \n",
    "        '''\n",
    "        temp = np.subtract(y_pred,Y)\n",
    "        weight = np.matmul(temp,X)/len(X)\n",
    "        bias = np.matmul(temp,np.ones((X.shape)))/len(X)\n",
    "        return (weight,bias)\n",
    "    \n",
    "    def cost(self,y_pred,Y):\n",
    "        error = np.sum(-np.multiply(Y,np.log(y_pred,where= y_pred !=0)))\n",
    "        return error\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        This function trains given dataset by iteratively updating their weights and bias.\n",
    "        weights is matrix of size (num of classes X num of words in Vocabulary)\n",
    "        bias is matrix of size (num of classes X 1)\n",
    "        \n",
    "        We divide dataset into mini-batches of 100 data instances and update weights and bias at end of each mini batch.\n",
    "        We repeat this process num_iter times\n",
    "        \n",
    "        '''\n",
    "        # weights initialization\n",
    "        self.num_class = Y.shape[1]\n",
    "        self.weights = np.zeros((Y.shape[1],X.shape[1]))\n",
    "        self.bias = np.zeros((Y.shape[1],X.shape[1]))\n",
    "        for i in range(self.num_iter):\n",
    "            ind = np.arange(len(X))\n",
    "            np.random.shuffle(ind)\n",
    "            b=np.arange(0,len(X),100)\n",
    "            np.append(b,len(X))\n",
    "            loss = 0\n",
    "            j=0\n",
    "            for j in range(len(b)-1):\n",
    "                x = X[ind[b[j]:b[j+1]]]\n",
    "                y = Y[ind[b[j]:b[j+1]]]\n",
    "                prob = np.zeros((2,len(x)))\n",
    "                prob = self.p(x)\n",
    "                error = self.cost(prob,y.T)\n",
    "                loss = loss + error/len(x)\n",
    "                change_in_w, change_in_b = self.gradient(prob,y.T,x)\n",
    "                self.weights = self.weights - self.eta*change_in_w\n",
    "                self.bias = self.bias - self.eta*change_in_b\n",
    "                \n",
    "            print(\"Loss : \",loss)\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def p(self, X): \n",
    "        '''\n",
    "        This function cpmputes probability for each data instance wrt each class.\n",
    "        \n",
    "        \"prob\" is a matrix of size ( num of data instances X num of classes )\n",
    "        For our particular case first case represent class \"nonoffensive\" and second column represent \"offensive\"\n",
    "        i.e prob[i][0] is probability for y=0 and prob[i][1] is probability for y=1 where y is actual output and\n",
    "        offensive means 1.\n",
    "        \n",
    "        '''\n",
    "        temp = np.add(np.matmul(self.weights,X.T),np.matmul(self.bias,np.ones((X.T.shape))))\n",
    "        prob = np.zeros((self.weights.shape))\n",
    "        prob = self.softmax(temp) \n",
    "        return prob\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        This function predicts the class to which given data instance belongs to.\n",
    "        We compare the calculated probability of given instance among all classes.\n",
    "        Class with max probability is the output.\n",
    "        \n",
    "        It also calculates F_1 score.\n",
    "        '''\n",
    "        (x_test,y_test) = featurize(X)\n",
    "        prob=self.p(x_test)\n",
    "        result=np.argmax(prob,axis=0)\n",
    "        tp=0\n",
    "        tn=0\n",
    "        fn=0\n",
    "        fp=0\n",
    "        y=y_test.T\n",
    "        print(y.shape)\n",
    "        print(y_test.shape)\n",
    "        for i in range(len(y_test)):\n",
    "            if(y[0][i]==result[i]):\n",
    "                if(result[i]==1):\n",
    "                    tp+=1\n",
    "                else:\n",
    "                    tn+=1\n",
    "            else:\n",
    "                if(result[i]==1):\n",
    "                    fp+=1\n",
    "                else:\n",
    "                    fn+=1\n",
    "        pr = tp/(tp+fp)\n",
    "        r = tp/(tp+fn)\n",
    "        f1 = 2*pr*r/(pr+r)\n",
    "        acc = (tp+tn)/(tp+tn+fn+fp)\n",
    "        print(\"F_1 score is: \"+str(f1))\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  4158.56323718056\n",
      "Loss :  2917.643189999999\n",
      "Loss :  1864.8614599999994\n",
      "Loss :  2127.621210000002\n",
      "Loss :  2823.9683599999926\n",
      "Loss :  513.2477199999928\n",
      "Loss :  3060.6792315205776\n",
      "Loss :  3972.5052738831682\n",
      "Loss :  5335.743563747172\n",
      "Loss :  1960.6762014031058\n",
      "Loss :  5167.948330175801\n",
      "Loss :  337.3188105009033\n",
      "Loss :  2948.4846683177734\n",
      "Loss :  2778.7250643881257\n",
      "Loss :  1383.7559333358859\n",
      "Loss :  1949.2901446115234\n",
      "Loss :  879.3674765007928\n",
      "Loss :  2941.404088271625\n",
      "Loss :  5198.309083337486\n",
      "Loss :  3877.834442125842\n",
      "Loss :  3109.841387686477\n",
      "Loss :  10.219883019361419\n",
      "Loss :  611.4186316294314\n",
      "Loss :  3652.4036804040834\n",
      "Loss :  2478.0501491471427\n",
      "Loss :  3683.275104726764\n",
      "Loss :  1777.9852560220588\n",
      "Loss :  1807.3878909404177\n",
      "Loss :  3701.4135159326006\n",
      "Loss :  4881.4282416555\n",
      "(2, 100)\n",
      "(100, 2)\n",
      "F_1 score is: 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "lg=LogReg(0.2,30)\n",
    "lg.train(x[:1000],y[:1000])\n",
    "lg.predict(test_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-281c098448a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-87-719612ddaaae>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                     \u001b[0mfn\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "lg.predict(test_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
