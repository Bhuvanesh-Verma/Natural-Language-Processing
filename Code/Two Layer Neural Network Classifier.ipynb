{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For your convenience, a function for reading in the dataset:\n",
    "import csv\n",
    "\n",
    "def load_dataset(filename):\n",
    "    intent = []\n",
    "    unique_intent = []\n",
    "    sentences = []\n",
    "    with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "        data = csv.reader(f, delimiter=\",\")\n",
    "        for row in data:\n",
    "            sentences.append(row[0])\n",
    "            intent.append(row[1])\n",
    "    unique_intent = set(intent)\n",
    "    return sentences, intent, unique_intent\n",
    "            \n",
    "sentences, intent, unique_intent = load_dataset(\"../Data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def create_bow_rep():\n",
    "    '''\n",
    "    This function computes required bag-of-words representation matrix.\n",
    "    \n",
    "    Output : bow is a (v x m) matrix where v is vocabulary size and m is number of examples. \n",
    "    \n",
    "    '''\n",
    "    bow = np.zeros((v,m))\n",
    "    for i in range(v):\n",
    "        word = list(vocab)[i]\n",
    "        for j in range(m):\n",
    "            if( word in sentences[j]):\n",
    "                \n",
    "                bow[i][j]=1\n",
    "            else:\n",
    "                bow[i][j]=0\n",
    "    return bow\n",
    "            \n",
    "def create_labelled_matrix():\n",
    "    '''\n",
    "    This function creates output matrix that is a hot vector representation of correct output for each example.\n",
    "    \n",
    "    Output : labeled is a (k x m) matrix where k is number of intent (total classes) and m is number of examples.\n",
    "    '''\n",
    "    labeled = np.zeros((k,m))\n",
    "    for i in range(k):\n",
    "        cls = list(unique_intent)[i]\n",
    "        for j in range(m):\n",
    "            if(cls == intent[j]):\n",
    "                \n",
    "                labeled[i][j]=1\n",
    "            else:\n",
    "                labeled[i][j]=0\n",
    "    \n",
    "    return labeled\n",
    "\n",
    "tokens_reg = re.compile(r\"[\\w']+|[.,!?;]\")\n",
    "tokens = [] #this list will contain tokens with punctuation seperated eg: \"help?\" is stored as \"help\" , \"?\" \n",
    "for word in sentences:\n",
    "    tokens+=tokens_reg.findall(word)\n",
    "vocab = set([word.lower() for word in tokens if word not in [\".\",\",\",\"!\",\"?\",\";\",\"[\",\"]\"]])\n",
    "m = len(sentences) # number of examples\n",
    "v = len(vocab) # vocabulary size\n",
    "k = len(unique_intent) # number of classes\n",
    "\n",
    "sentc_2_ix = {sentc:sentences.index(sentc) for sentc in sentences} \n",
    "vocab_2_ix = {word:list(vocab).index(word) for word in list(vocab)}\n",
    "intent_2_ix = {intent:list(unique_intent).index(intent) for intent in list(unique_intent)}\n",
    "\n",
    "X = create_bow_rep()\n",
    "Y = create_labelled_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNNClassifier:\n",
    "    \n",
    "    def __init__(self,hidden,classes, eta=0.005, epoch=100, seed = 5 ):\n",
    "        self.eta = eta # Learning rate \n",
    "        self.epoch = epoch\n",
    "        self.seed = seed # Tuning initial parameter values\n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        self.hidden_z = None\n",
    "        self.hidden = hidden # Number of hidden layer neurons\n",
    "        self.classes = classes #Number of classes\n",
    "        \n",
    "    \n",
    "    def initialise_parameters(self,X):\n",
    "        '''\n",
    "        This function initialise parameters like Weight and bias matrix. \n",
    "\n",
    "        W is weight matrix between Input and Hidden Layer of shape (150 x v) where v is vocabulary size\n",
    "        b is bias matrix between Input and Hidden Layer of shape (150 x m ) where m is number of examples\n",
    "        U is weight matrix between Hidden and Output Layer of shape (k x 150) where k is number of intents/classes\n",
    "        There is no bias term between Hidden and Output Layer\n",
    "\n",
    "        Input : seed value for random state\n",
    "\n",
    "        Output : Weight and bias matrix\n",
    "\n",
    "        '''\n",
    "        v,n = X.shape\n",
    "        rang = np.random.RandomState(seed=self.seed)\n",
    "        a=rang.uniform(-1,1,size=(self.hidden,v))\n",
    "        b = rang.uniform(-1,1,size=(self.hidden,n))\n",
    "        c=rang.uniform(-1,1,size=(self.classes,self.hidden))\n",
    "        d = rang.uniform(-1,1,size=(self.classes,n))\n",
    "        self.weights[0] = np.array(a, dtype=np.float64)\n",
    "        self.bias[0] = np.array(b, dtype=np.float64)\n",
    "        self.weights[1] = np.array(c, dtype=np.float64)\n",
    "        self.bias[1] = np.array(d, dtype=np.float64)\n",
    "\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        '''\n",
    "            This function is forward feed for neural network. It calculates activation at hidden layer using ReLU and at output layer\n",
    "            using softmax.\n",
    "\n",
    "            Output : It returns a tuple containing three entities : \n",
    "                     y_out : It is a matrix of shape (k x m) where k is number of classes/intents and m is number of examples.\n",
    "                             It contains predicted probability distribution of each examples over k classes. \n",
    "\n",
    "                     z_at_hidden : It is matrix of shape (150 x m) where m is number of examples. It is weighted sum of input at \n",
    "                                   hidden layer for each example. Every column vector represent weighted sum for each neuron  at\n",
    "                                   hidden layer.\n",
    "\n",
    "                     activation_at_hidden : It is matrix of shape (150 x m). It is activation value for each neuron at hidden layer\n",
    "                                             wrt every example. Every column vector represent activation value for each neuron at\n",
    "                                             hidden layer.\n",
    "\n",
    "            '''\n",
    "\n",
    "        #Propogation to Hidden Layer\n",
    "        self.hidden_z = np.add(np.matmul(self.weights[0],X),\n",
    "                               np.matmul(self.bias[0],np.ones((self.bias[0].shape[1],X.shape[1]))/self.bias[0].shape[1])) # weighted sum of input at hidden layer\n",
    "        activation_at_hidden = self.relu(self.hidden_z) # activation matrix at hidden layer\n",
    "\n",
    "        #Propogation to output layer\n",
    "        z_at_output = np.add(np.matmul(self.weights[1],activation_at_hidden),\n",
    "                            np.matmul(self.bias[1],np.ones((self.bias[1].shape[1],\n",
    "                                                            activation_at_hidden.shape[1]))/self.bias[1].shape[1])) # weighted sum at output\n",
    "        y_pred = self.softmax(z_at_output) # probability distribution over different intents for each example\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def backward(self,X,Y,y_pred):\n",
    "        \n",
    "        '''\n",
    "        This function back proporgates the error to previous layers. It calculates error terms for each layer and then calculates\n",
    "        partial derivatives wrt weights and bias of every layer.\n",
    "\n",
    "        Input : y_out : It is a matrix of shape (k x m) where k is number of classes/intents and m is number of examples.\n",
    "                         It contains predicted probability distribution of each examples over k classes. \n",
    "\n",
    "                z_at_hidden : It is matrix of shape (150 x m) where m is number of examples. It is weighted sum of input at \n",
    "                               hidden layer for each example. Every column vector represent weighted sum for each neuron  at\n",
    "                               hidden layer.\n",
    "\n",
    "                activation_at_hidden : It is matrix of shape (150 x m). It is activation value for each neuron at hidden layer\n",
    "                                         wrt every example. Every column vector represent activation value for each neuron at\n",
    "                                         hidden layer.\n",
    "        Output : It returns a tuple contaning change in parameters,\n",
    "                 del_weight_of_U : This is a (k x 150) matrix containing change in each weight between hidden and output layer.\n",
    "                 del_weight_of_W : This is a (150 x v) matrix containing change in each weight between input and hidden layer.\n",
    "                 del_bias : This is a (150 x m) matrix containing change in each bias term between input and hidden layer.\n",
    "\n",
    "        '''\n",
    "        y_true = np.array(Y)\n",
    "        del_error_at_out = y_pred - y_true # error term at output layer\n",
    "        del_error_at_hidden = np.multiply(np.matmul(self.weights[1].T,del_error_at_out),self.deriv_relu(self.hidden_z)) # error term at hidden layer\n",
    "        partial_change_in_weight_of_U = np.matmul(del_error_at_out,self.relu(self.hidden_z).T) # weights between hidden and output layer\n",
    "        partial_change_in_weight_of_W = np.matmul(del_error_at_hidden,X.T) # weights between input and hidden layer\n",
    "        partial_change_in_bias_of_W = del_error_at_hidden # bias for hidden layer neurons\n",
    "        partial_change_in_bias_of_U = del_error_at_out\n",
    "        del_weight_of_U = partial_change_in_weight_of_U\n",
    "        del_weight_of_W = partial_change_in_weight_of_W\n",
    "        del_bias_of_W = partial_change_in_bias_of_W\n",
    "        del_bias_of_U = partial_change_in_bias_of_U\n",
    "        return (del_weight_of_U,del_weight_of_W,del_bias_of_W,del_bias_of_U)\n",
    "    \n",
    "    \n",
    "    def train(self,X,Y,batch):\n",
    "        m = X.shape[1]\n",
    "        for u in range(self.epoch):\n",
    "            cost_list = []\n",
    "            indices = np.arange(m)\n",
    "            np.random.shuffle(indices) # array with shuffled indicies of examples\n",
    "            j=np.arange(0,m,batch) \n",
    "            np.append(j,m-1)\n",
    "            for i in range(len(j)-1):\n",
    "\n",
    "                #Parameters initialization\n",
    "                del_U = 0\n",
    "                del_W = 0\n",
    "                del_b1 = 0\n",
    "                del_b2 = 0\n",
    "\n",
    "                #Batch Data\n",
    "                x = X[:,indices[j[i]:j[i+1]]]\n",
    "                y = Y[:,indices[j[i]:j[i+1]]]\n",
    "\n",
    "                #Forward Feed\n",
    "                y_pred = self.forward(x)\n",
    "\n",
    "                # Cost\n",
    "                cost = self.cost_func(y,y_pred)/len(j)\n",
    "                cost_list.append(cost)\n",
    "\n",
    "                #Backward Feed\n",
    "                (del_U,del_W,del_b1,del_b2) = self.backward(x,y,y_pred)\n",
    "                del_U = del_U/len(j)\n",
    "                del_W = del_W/len(j)\n",
    "                del_b1 = np.matmul(del_b1,np.ones((del_b1.shape[1],m)))/len(j)\n",
    "                del_b2 = np.matmul(del_b2,np.ones((del_b2.shape[1],m)))/len(j)\n",
    "\n",
    "                #Parameters update\n",
    "                self.weights[1] -= (self.eta)*del_U\n",
    "                self.weights[0] -= (self.eta)*del_W\n",
    "                self.bias[0] -= (self.eta)*del_b1\n",
    "                self.bias[1] -= (self.eta)*del_b2\n",
    "            print(\"Loss is \",sum(cost_list))\n",
    "    \n",
    "    def cost_func(self,Y,y_pred):\n",
    "        '''\n",
    "        This function computes average loss over whole set of examples.\n",
    "\n",
    "        Input : y_out is matrix of shape (k x m) which contain predicted values for every example. Every column vector,\n",
    "                representing an example, contains predicted probability distribution over k classes.\n",
    "\n",
    "        Output : return average cost over whole dataset\n",
    "\n",
    "        '''\n",
    "        y_true = np.array(Y) # converting labelled example into numpy matrix\n",
    "\n",
    "        # For every column vector in matrix of labelled example, we retrieve index of maximum value, which is 1 in this case,\n",
    "        # This index corresponds to correct class for that particular example.\n",
    "        true_index = np.argmax(y_true,axis=0)\n",
    "\n",
    "        # Selecting only those predictions from each column which corresponds to correct class for that example. Since row\n",
    "        # in y_out represent classes and column represent examples, so for every column we are choosing a row index from \"true_index\"\n",
    "        # vector which contain row index of correct class.\n",
    "        cost = np.sum(-np.log(y_pred[true_index,np.arange(y_true.shape[1])]))\n",
    "        return cost\n",
    "\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        '''\n",
    "        This function computes softmax value for an array or a matrix\n",
    "\n",
    "        Input : x is a array or a matrix\n",
    "\n",
    "        Output : result is either a array of softmax value or a matrix\n",
    "        '''\n",
    "        x=x.astype(float)\n",
    "        if x.ndim==1:\n",
    "            return np.exp(x)/np.sum(np.exp(x))\n",
    "        elif x.ndim==2:\n",
    "            result=np.zeros_like(x)\n",
    "            M,N=x.shape\n",
    "            z = x - np.max(x, axis=0, keepdims=True)\n",
    "            for n in range(N):\n",
    "                S=np.sum(np.exp(z[:,n]))\n",
    "                result[:,n]=np.exp(z[:,n])/S\n",
    "            return result\n",
    "        else:\n",
    "            print(\"The input array is not 1- or 2-dimensional.\")\n",
    "        \n",
    "    def relu(self,X):\n",
    "        '''\n",
    "        This function compares each value in given array or matrix with 0 and returns maximum of that comparison\n",
    "\n",
    "        Input : Array or matrix of real numbers\n",
    "\n",
    "        Output : Array or matrix of real numbers\n",
    "        '''\n",
    "        return np.maximum(0,X)\n",
    "    \n",
    "    def deriv_relu(self,X):\n",
    "        '''\n",
    "        This function returns derivative of relu() function. It compares each value in array or matrix with 0 and returns\n",
    "        1 if a value is greater than 0 else it returns 0.\n",
    "\n",
    "        Input : Array or matrix of real numbers\n",
    "\n",
    "        Output : Array or matrix of 0's and 1's\n",
    "        '''\n",
    "        return np.where(X<=0,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X,Y,classifier):\n",
    "    '''\n",
    "    This function computes accuracy of model by computing ratio of number of correct prediction to number of examples.\n",
    "    \n",
    "    Output : a real number between 0 and 1, where 1 being highest accuracy.\n",
    "    '''\n",
    "    y = classifier.forward(X) # predicted values\n",
    "    \n",
    "    # For every column vector in matrix of predicted value, we retrieve index of maximum value in that vector. This index \n",
    "    # correspond to predicted class index.\n",
    "    predicted = np.argmax(y,axis=0) # predicted index (intent) of maximum value for each example\n",
    "    \n",
    "    true_output = np.array(Y) # Converting labelled examples to numpy matrix\n",
    "    \n",
    "    # For every column vector in matrix of labelled example, we retrieve index of maximum value, which is 1 in this case,\n",
    "    # This index corresponds to correct class for that particular example.\n",
    "    true = np.argmax(true_output,axis=0) # true intent vector for each example\n",
    "    \n",
    "    \n",
    "    accuracy = (np.sum(predicted == true))/m\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is  1275.1792604571938\n",
      "Loss is  854.1413184222484\n",
      "Loss is  670.6046780230868\n",
      "Loss is  550.353140892975\n",
      "Loss is  464.17573535951385\n",
      "Loss is  402.330287015827\n",
      "Loss is  359.2311782219456\n",
      "Loss is  317.22143159595277\n",
      "Loss is  289.75433468704597\n",
      "Loss is  265.149138629358\n",
      "Loss is  242.7654376163639\n",
      "Loss is  225.20865293215618\n",
      "Loss is  208.06307300457752\n",
      "Loss is  194.51902513887194\n",
      "Loss is  181.9353955934242\n",
      "Loss is  169.71548368071916\n",
      "Loss is  160.0967172749584\n",
      "Loss is  151.8116786859517\n",
      "Loss is  142.52719817354557\n",
      "Loss is  135.37320824125817\n",
      "Loss is  127.28911949567706\n",
      "Loss is  121.11007474879894\n",
      "Loss is  114.78842487288864\n",
      "Loss is  109.24533375802456\n",
      "Loss is  105.04654751694218\n",
      "Loss is  99.58458856583857\n",
      "Loss is  95.65605889122426\n",
      "Loss is  91.89934976916919\n",
      "Loss is  87.40606666226017\n",
      "Loss is  83.32146721939131\n",
      "Loss is  79.761576265081\n",
      "Loss is  77.4519219486899\n",
      "Loss is  73.02701741677573\n",
      "Loss is  70.42196536928772\n",
      "Loss is  68.16060997856097\n",
      "Loss is  65.92236181741015\n",
      "Loss is  61.76442246168888\n",
      "Loss is  60.91577475903048\n",
      "Loss is  58.78825196108682\n",
      "Loss is  56.000936516244366\n",
      "Loss is  54.05478577053428\n",
      "Loss is  52.001318462401905\n",
      "Loss is  50.05950616511752\n",
      "Loss is  48.949931267698894\n",
      "Loss is  47.15478433368443\n",
      "Loss is  45.62057975524228\n",
      "Loss is  43.858859180314255\n",
      "Loss is  42.742232453675804\n",
      "Loss is  40.81073616742717\n",
      "Loss is  39.77887158136799\n",
      "Loss is  38.23555001417608\n",
      "Loss is  37.01548251877653\n",
      "Loss is  36.251430272737686\n",
      "Loss is  34.86840453580139\n",
      "Loss is  33.311620926613536\n",
      "Loss is  32.26071533188382\n",
      "Loss is  31.659352763900905\n",
      "Loss is  29.759093903249788\n",
      "Loss is  29.936691899236333\n",
      "Loss is  29.313930080517174\n",
      "Loss is  28.43979922063253\n",
      "Loss is  27.039180910204657\n",
      "Loss is  26.828774628744785\n",
      "Loss is  26.026962630591186\n",
      "Loss is  25.083370421849942\n",
      "Loss is  24.621317786263933\n",
      "Loss is  23.66405662090714\n",
      "Loss is  23.1655842368803\n",
      "Loss is  22.24254916949592\n",
      "Loss is  21.966561320348372\n",
      "Loss is  20.959873953741674\n",
      "Loss is  20.49401969141907\n",
      "Loss is  20.069419899106705\n",
      "Loss is  19.838472154180998\n",
      "Loss is  19.2631422676384\n",
      "Loss is  18.74857681725751\n",
      "Loss is  17.80270500155206\n",
      "Loss is  17.77734715395052\n",
      "Loss is  17.131242324386374\n",
      "Loss is  16.5637931596614\n",
      "Loss is  16.504846892961577\n",
      "Loss is  16.049581834649565\n",
      "Loss is  15.682091206247296\n",
      "Loss is  15.391434208442362\n",
      "Loss is  14.959498478400082\n",
      "Loss is  14.663003282469269\n",
      "Loss is  14.372899854180261\n",
      "Loss is  13.478151085808394\n",
      "Loss is  13.6344221020021\n",
      "Loss is  13.403466003144336\n",
      "Loss is  13.01596271687433\n",
      "Loss is  12.902047712820927\n",
      "Loss is  12.607481522450874\n",
      "Loss is  12.366824649135888\n",
      "Loss is  12.053299078080283\n",
      "Loss is  11.769385805171439\n",
      "Loss is  11.56928642305139\n",
      "Loss is  11.30064219829041\n",
      "Loss is  10.835301043859038\n",
      "Loss is  10.780125188269025\n",
      "Loss is  10.626797267127241\n",
      "Loss is  10.373730555689013\n",
      "Loss is  10.37885056751171\n",
      "Loss is  10.16773967005119\n",
      "Loss is  9.855245163008622\n",
      "Loss is  9.741814893090595\n",
      "Loss is  9.51073817664821\n",
      "Loss is  9.494986462920703\n",
      "Loss is  9.183929799745492\n",
      "Loss is  8.963424445641147\n",
      "Loss is  9.024561260808099\n",
      "Loss is  8.909400544230797\n",
      "Loss is  8.674386606956054\n",
      "Loss is  8.186522783536176\n",
      "Loss is  8.347847002907555\n",
      "Loss is  8.132032296492824\n",
      "Loss is  8.213305350909268\n",
      "Loss is  8.053226192790575\n",
      "Loss is  7.969933058223434\n",
      "Loss is  7.86271588492395\n",
      "Loss is  7.672857905415221\n",
      "Loss is  7.5660943843199515\n",
      "Loss is  7.559976365936842\n",
      "Loss is  7.36793682604546\n",
      "Loss is  7.307342362105227\n",
      "Loss is  7.14845770560281\n",
      "Loss is  6.959093325414789\n",
      "Loss is  7.01389861103757\n",
      "Loss is  6.699727840565987\n",
      "Loss is  6.827385333042929\n",
      "Loss is  6.76612694045163\n",
      "Loss is  6.653720472794676\n",
      "Loss is  6.428699679713969\n",
      "Loss is  6.545599268689913\n",
      "Loss is  6.476757990161757\n",
      "Loss is  6.375028316618328\n",
      "Loss is  6.229087266403722\n",
      "Loss is  6.144339048236253\n",
      "Loss is  6.13323840989178\n",
      "Loss is  6.068177218970732\n",
      "Loss is  5.963803508279465\n",
      "Loss is  5.919419363709161\n",
      "Loss is  5.896281329919683\n",
      "Loss is  5.8320851355350545\n",
      "Loss is  5.766736383374303\n",
      "Loss is  5.707885380327345\n",
      "Loss is  5.6945872850969845\n",
      "Loss is  5.598466711140313\n",
      "Loss is  5.428760159676635\n",
      "Loss is  5.506657280829497\n",
      "Loss is  5.353760420238841\n",
      "Loss is  5.413474319022774\n",
      "Loss is  5.384586964751201\n",
      "Loss is  5.2795771159240985\n",
      "Loss is  5.264436087142024\n",
      "Loss is  5.147650712471814\n",
      "Loss is  5.151221339519523\n",
      "Loss is  5.090271080114128\n",
      "Loss is  5.045196436632449\n",
      "Loss is  4.925346137783272\n",
      "Loss is  5.012976627522586\n",
      "Loss is  4.902620119313423\n",
      "Loss is  4.892977399287776\n",
      "Loss is  4.8290578079282405\n",
      "Loss is  4.803778536343462\n",
      "Loss is  4.501370965483186\n",
      "Loss is  4.730765092750833\n",
      "Loss is  4.702616592681306\n",
      "Loss is  4.6688195448868655\n",
      "Loss is  4.595065098936388\n",
      "Loss is  4.571653709834377\n",
      "Loss is  4.543011244953957\n",
      "Loss is  4.504717320211489\n",
      "Loss is  4.481458620346557\n",
      "Loss is  4.3906320775289105\n",
      "Loss is  4.346022925452504\n",
      "Loss is  4.274318800038754\n",
      "Loss is  4.38795108230598\n",
      "Loss is  4.118493624844977\n",
      "Loss is  4.0913557475997235\n",
      "Loss is  4.238720185657832\n",
      "Loss is  4.162280881246267\n",
      "Loss is  4.2359657328463545\n",
      "Loss is  4.06055290586535\n",
      "Loss is  4.165773234071593\n",
      "Loss is  4.144566363301037\n",
      "Loss is  4.052044065100889\n",
      "Loss is  4.014875892461381\n",
      "Loss is  4.087294976357496\n",
      "Loss is  4.030685547696677\n",
      "Loss is  4.016465336671178\n",
      "Loss is  3.9966837236692387\n",
      "Loss is  3.9303828303239765\n",
      "Loss is  3.8622976181322994\n",
      "Loss is  3.881811059534934\n",
      "Loss is  3.8401744857334306\n",
      "Loss is  3.8814932001903557\n",
      "Loss is  3.8532545147490893\n",
      "Loss is  3.784285717236008\n",
      "Loss is  3.762378265607372\n"
     ]
    }
   ],
   "source": [
    "classifier = TwoLayerNNClassifier(150,k,epoch=200)\n",
    "classifier.initialise_parameters(X)\n",
    "classifier.train(X,Y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9928122192273136"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(X,Y,classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
