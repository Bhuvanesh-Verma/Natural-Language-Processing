{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For your convenience, a function for reading in the dataset:\n",
    "import csv\n",
    "\n",
    "def load_dataset(filename):\n",
    "    intent = []\n",
    "    unique_intent = []\n",
    "    sentences = []\n",
    "    with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "        data = csv.reader(f, delimiter=\",\")\n",
    "        for row in data:\n",
    "            sentences.append(row[0])\n",
    "            intent.append(row[1])\n",
    "    unique_intent = set(intent)\n",
    "    return sentences, intent, unique_intent\n",
    "            \n",
    "sentences, intent, unique_intent = load_dataset(\"../Data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def create_bow_rep():\n",
    "    '''\n",
    "    This function computes required bag-of-words representation matrix.\n",
    "    \n",
    "    Output : bow is a (v x m) matrix where v is vocabulary size and m is number of examples. \n",
    "    \n",
    "    '''\n",
    "    bow = np.zeros((v,m))\n",
    "    for i in range(v):\n",
    "        word = list(vocab)[i]\n",
    "        for j in range(m):\n",
    "            if( word in sentences[j]):\n",
    "                \n",
    "                bow[i][j]=1\n",
    "            else:\n",
    "                bow[i][j]=0\n",
    "    return bow\n",
    "            \n",
    "def create_labelled_matrix():\n",
    "    '''\n",
    "    This function creates output matrix that is a hot vector representation of correct output for each example.\n",
    "    \n",
    "    Output : labeled is a (k x m) matrix where k is number of intent (total classes) and m is number of examples.\n",
    "    '''\n",
    "    labeled = np.zeros((k,m))\n",
    "    for i in range(k):\n",
    "        cls = list(unique_intent)[i]\n",
    "        for j in range(m):\n",
    "            if(cls == intent[j]):\n",
    "                \n",
    "                labeled[i][j]=1\n",
    "            else:\n",
    "                labeled[i][j]=0\n",
    "    \n",
    "    return labeled\n",
    "\n",
    "tokens_reg = re.compile(r\"[\\w']+|[.,!?;]\")\n",
    "tokens = [] #this list will contain tokens with punctuation seperated eg: \"help?\" is stored as \"help\" , \"?\" \n",
    "for word in sentences:\n",
    "    tokens+=tokens_reg.findall(word)\n",
    "vocab = set([word.lower() for word in tokens if word not in [\".\",\",\",\"!\",\"?\",\";\",\"[\",\"]\"]])\n",
    "m = len(sentences) # number of examples\n",
    "v = len(vocab) # vocabulary size\n",
    "k = len(unique_intent) # number of classes\n",
    "\n",
    "sentc_2_ix = {sentc:sentences.index(sentc) for sentc in sentences} \n",
    "vocab_2_ix = {word:list(vocab).index(word) for word in list(vocab)}\n",
    "intent_2_ix = {intent:list(unique_intent).index(intent) for intent in list(unique_intent)}\n",
    "\n",
    "X = create_bow_rep()\n",
    "Y = create_labelled_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNNClassifier:\n",
    "    \n",
    "    def __init__(self,hidden,classes, eta=0.005, epoch=100, seed = 5 ):\n",
    "        self.eta = eta # Learning rate \n",
    "        self.epoch = epoch\n",
    "        self.seed = seed # Tuning initial parameter values\n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        self.hidden_z = None\n",
    "        self.hidden = hidden # Number of hidden layer neurons\n",
    "        self.classes = classes #Number of classes\n",
    "        \n",
    "    \n",
    "    def initialise_parameters(self,X):\n",
    "        '''\n",
    "        This function initialise parameters like Weight and bias matrix. \n",
    "\n",
    "        W is weight matrix between Input and Hidden Layer of shape (150 x v) where v is vocabulary size\n",
    "        b is bias matrix between Input and Hidden Layer of shape (150 x m ) where m is number of examples\n",
    "        U is weight matrix between Hidden and Output Layer of shape (k x 150) where k is number of intents/classes\n",
    "        There is no bias term between Hidden and Output Layer\n",
    "\n",
    "        Input : seed value for random state\n",
    "\n",
    "        Output : Weight and bias matrix\n",
    "\n",
    "        '''\n",
    "        v,n = X.shape\n",
    "        rang = np.random.RandomState(seed=self.seed)\n",
    "        a=rang.uniform(-1,1,size=(self.hidden,v))\n",
    "        b = rang.uniform(-1,1,size=(self.hidden,1))\n",
    "        c=rang.uniform(-1,1,size=(self.classes,self.hidden))\n",
    "        d = rang.uniform(-1,1,size=(self.classes,1))\n",
    "        self.weights[0] = np.array(a, dtype=np.float64)\n",
    "        self.bias[0] = np.array(b, dtype=np.float64)\n",
    "        self.weights[1] = np.array(c, dtype=np.float64)\n",
    "        self.bias[1] = np.array(d, dtype=np.float64)\n",
    "\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        '''\n",
    "            This function is forward feed for neural network. It calculates activation at hidden layer using ReLU and at output layer\n",
    "            using softmax.\n",
    "\n",
    "            Output : It returns a tuple containing three entities : \n",
    "                     y_out : It is a matrix of shape (k x m) where k is number of classes/intents and m is number of examples.\n",
    "                             It contains predicted probability distribution of each examples over k classes. \n",
    "\n",
    "                     z_at_hidden : It is matrix of shape (150 x m) where m is number of examples. It is weighted sum of input at \n",
    "                                   hidden layer for each example. Every column vector represent weighted sum for each neuron  at\n",
    "                                   hidden layer.\n",
    "\n",
    "                     activation_at_hidden : It is matrix of shape (150 x m). It is activation value for each neuron at hidden layer\n",
    "                                             wrt every example. Every column vector represent activation value for each neuron at\n",
    "                                             hidden layer.\n",
    "\n",
    "            '''\n",
    "\n",
    "        #Propogation to Hidden Layer\n",
    "        self.hidden_z = np.add(np.matmul(self.weights[0],X),\n",
    "                               np.matmul(self.bias[0],np.ones((self.bias[0].shape[1],X.shape[1])))) # weighted sum of input at hidden layer\n",
    "        activation_at_hidden = self.relu(self.hidden_z) # activation matrix at hidden layer\n",
    "\n",
    "        #Propogation to output layer\n",
    "        z_at_output = np.add(np.matmul(self.weights[1],activation_at_hidden),\n",
    "                            np.matmul(self.bias[1],np.ones((self.bias[1].shape[1],\n",
    "                                                            activation_at_hidden.shape[1])))) # weighted sum at output\n",
    "        y_pred = self.softmax(z_at_output) # probability distribution over different intents for each example\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def backward(self,X,Y,y_pred):\n",
    "        \n",
    "        '''\n",
    "        This function back proporgates the error to previous layers. It calculates error terms for each layer and then calculates\n",
    "        partial derivatives wrt weights and bias of every layer.\n",
    "\n",
    "        Input : y_out : It is a matrix of shape (k x m) where k is number of classes/intents and m is number of examples.\n",
    "                         It contains predicted probability distribution of each examples over k classes. \n",
    "\n",
    "                z_at_hidden : It is matrix of shape (150 x m) where m is number of examples. It is weighted sum of input at \n",
    "                               hidden layer for each example. Every column vector represent weighted sum for each neuron  at\n",
    "                               hidden layer.\n",
    "\n",
    "                activation_at_hidden : It is matrix of shape (150 x m). It is activation value for each neuron at hidden layer\n",
    "                                         wrt every example. Every column vector represent activation value for each neuron at\n",
    "                                         hidden layer.\n",
    "        Output : It returns a tuple contaning change in parameters,\n",
    "                 del_weight_of_U : This is a (k x 150) matrix containing change in each weight between hidden and output layer.\n",
    "                 del_weight_of_W : This is a (150 x v) matrix containing change in each weight between input and hidden layer.\n",
    "                 del_bias : This is a (150 x m) matrix containing change in each bias term between input and hidden layer.\n",
    "\n",
    "        '''\n",
    "        y_true = np.array(Y)\n",
    "        del_error_at_out = y_pred - y_true # error term at output layer\n",
    "        del_error_at_hidden = np.multiply(np.matmul(self.weights[1].T,del_error_at_out),self.deriv_relu(self.hidden_z)) # error term at hidden layer\n",
    "        partial_change_in_weight_of_U = np.matmul(del_error_at_out,self.relu(self.hidden_z).T) # weights between hidden and output layer\n",
    "        partial_change_in_weight_of_W = np.matmul(del_error_at_hidden,X.T) # weights between input and hidden layer\n",
    "        partial_change_in_bias_of_W = del_error_at_hidden # bias for hidden layer neurons\n",
    "        partial_change_in_bias_of_U = del_error_at_out\n",
    "        del_weight_of_U = partial_change_in_weight_of_U\n",
    "        del_weight_of_W = partial_change_in_weight_of_W\n",
    "        del_bias_of_W = partial_change_in_bias_of_W\n",
    "        del_bias_of_U = partial_change_in_bias_of_U\n",
    "        return (del_weight_of_U,del_weight_of_W,del_bias_of_W,del_bias_of_U)\n",
    "    \n",
    "    \n",
    "    def train(self,X,Y,batch):\n",
    "        m = X.shape[1]\n",
    "        for u in range(self.epoch):\n",
    "            cost_list = []\n",
    "            indices = np.arange(m)\n",
    "            np.random.shuffle(indices) # array with shuffled indicies of examples\n",
    "            j=np.arange(0,m,batch) \n",
    "            np.append(j,m-1)\n",
    "            for i in range(len(j)-1):\n",
    "\n",
    "                #Parameters initialization\n",
    "                del_U = 0\n",
    "                del_W = 0\n",
    "                del_b1 = 0\n",
    "                del_b2 = 0\n",
    "\n",
    "                #Batch Data\n",
    "                x = X[:,indices[j[i]:j[i+1]]]\n",
    "                y = Y[:,indices[j[i]:j[i+1]]]\n",
    "\n",
    "                #Forward Feed\n",
    "                y_pred = self.forward(x)\n",
    "\n",
    "                # Cost\n",
    "                cost = self.cost_func(y,y_pred)/len(j)\n",
    "                cost_list.append(cost)\n",
    "\n",
    "                #Backward Feed\n",
    "                (del_U,del_W,del_b1,del_b2) = self.backward(x,y,y_pred)\n",
    "                del_U = del_U/len(j)\n",
    "                del_W = del_W/len(j)\n",
    "                del_b1 = (np.mean(del_b1,axis=1)).reshape(-1,1)\n",
    "                del_b2 = (np.mean(del_b2,axis=1)).reshape(-1,1)\n",
    "\n",
    "                #Parameters update\n",
    "                self.weights[1] -= (self.eta)*del_U\n",
    "                self.weights[0] -= (self.eta)*del_W\n",
    "                self.bias[0] -= (self.eta)*del_b1\n",
    "                self.bias[1] -= (self.eta)*del_b2\n",
    "            print(\"Loss is \",sum(cost_list))\n",
    "    \n",
    "    def cost_func(self,Y,y_pred):\n",
    "        '''\n",
    "        This function computes average loss over whole set of examples.\n",
    "\n",
    "        Input : y_out is matrix of shape (k x m) which contain predicted values for every example. Every column vector,\n",
    "                representing an example, contains predicted probability distribution over k classes.\n",
    "\n",
    "        Output : return average cost over whole dataset\n",
    "\n",
    "        '''\n",
    "        y_true = np.array(Y) # converting labelled example into numpy matrix\n",
    "\n",
    "        # For every column vector in matrix of labelled example, we retrieve index of maximum value, which is 1 in this case,\n",
    "        # This index corresponds to correct class for that particular example.\n",
    "        true_index = np.argmax(y_true,axis=0)\n",
    "\n",
    "        # Selecting only those predictions from each column which corresponds to correct class for that example. Since row\n",
    "        # in y_out represent classes and column represent examples, so for every column we are choosing a row index from \"true_index\"\n",
    "        # vector which contain row index of correct class.\n",
    "        cost = np.sum(-np.log(y_pred[true_index,np.arange(y_true.shape[1])]))\n",
    "        return cost\n",
    "\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        '''\n",
    "        This function computes softmax value for an array or a matrix\n",
    "\n",
    "        Input : x is a array or a matrix\n",
    "\n",
    "        Output : result is either a array of softmax value or a matrix\n",
    "        '''\n",
    "        x=x.astype(float)\n",
    "        if x.ndim==1:\n",
    "            return np.exp(x)/np.sum(np.exp(x))\n",
    "        elif x.ndim==2:\n",
    "            result=np.zeros_like(x)\n",
    "            M,N=x.shape\n",
    "            z = x - np.max(x, axis=0, keepdims=True)\n",
    "            for n in range(N):\n",
    "                S=np.sum(np.exp(z[:,n]))\n",
    "                result[:,n]=np.exp(z[:,n])/S\n",
    "            return result\n",
    "        else:\n",
    "            print(\"The input array is not 1- or 2-dimensional.\")\n",
    "        \n",
    "    def relu(self,X):\n",
    "        '''\n",
    "        This function compares each value in given array or matrix with 0 and returns maximum of that comparison\n",
    "\n",
    "        Input : Array or matrix of real numbers\n",
    "\n",
    "        Output : Array or matrix of real numbers\n",
    "        '''\n",
    "        return np.maximum(0,X)\n",
    "    \n",
    "    def deriv_relu(self,X):\n",
    "        '''\n",
    "        This function returns derivative of relu() function. It compares each value in array or matrix with 0 and returns\n",
    "        1 if a value is greater than 0 else it returns 0.\n",
    "\n",
    "        Input : Array or matrix of real numbers\n",
    "\n",
    "        Output : Array or matrix of 0's and 1's\n",
    "        '''\n",
    "        return np.where(X<=0,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X,Y,classifier):\n",
    "    '''\n",
    "    This function computes accuracy of model by computing ratio of number of correct prediction to number of examples.\n",
    "    \n",
    "    Output : a real number between 0 and 1, where 1 being highest accuracy.\n",
    "    '''\n",
    "    y = classifier.forward(X) # predicted values\n",
    "    \n",
    "    # For every column vector in matrix of predicted value, we retrieve index of maximum value in that vector. This index \n",
    "    # correspond to predicted class index.\n",
    "    predicted = np.argmax(y,axis=0) # predicted index (intent) of maximum value for each example\n",
    "    \n",
    "    true_output = np.array(Y) # Converting labelled examples to numpy matrix\n",
    "    \n",
    "    # For every column vector in matrix of labelled example, we retrieve index of maximum value, which is 1 in this case,\n",
    "    # This index corresponds to correct class for that particular example.\n",
    "    true = np.argmax(true_output,axis=0) # true intent vector for each example\n",
    "    \n",
    "    \n",
    "    accuracy = (np.sum(predicted == true))/m\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is  1198.6853557306447\n",
      "Loss is  794.4781524920709\n",
      "Loss is  641.6800501027211\n",
      "Loss is  538.032175690774\n",
      "Loss is  468.43170538381673\n",
      "Loss is  413.2481374734132\n",
      "Loss is  369.225287681886\n",
      "Loss is  337.2451179530668\n",
      "Loss is  305.84282966024404\n",
      "Loss is  282.1370649070438\n",
      "Loss is  262.73304824051104\n",
      "Loss is  242.00615086862467\n",
      "Loss is  227.9947132732362\n",
      "Loss is  212.3307978705223\n",
      "Loss is  198.0040532833391\n",
      "Loss is  189.94191593520713\n",
      "Loss is  178.86217914308722\n",
      "Loss is  169.0637165563023\n",
      "Loss is  161.2122407440949\n",
      "Loss is  154.74469790070924\n",
      "Loss is  147.1448781803512\n",
      "Loss is  139.9645981375958\n",
      "Loss is  133.57090940753596\n",
      "Loss is  129.6620490830562\n",
      "Loss is  124.38147428928198\n",
      "Loss is  119.25534791896185\n",
      "Loss is  114.73502016763948\n",
      "Loss is  109.3017113105007\n",
      "Loss is  106.90537652800904\n",
      "Loss is  101.3120588178549\n",
      "Loss is  97.34975013634583\n",
      "Loss is  94.87665301553037\n",
      "Loss is  92.6699659387588\n",
      "Loss is  88.05765436579578\n",
      "Loss is  86.33213988225786\n",
      "Loss is  82.50154923302192\n",
      "Loss is  79.44745997419191\n",
      "Loss is  77.89846794411471\n",
      "Loss is  75.58252859261736\n",
      "Loss is  71.5561402451329\n",
      "Loss is  70.26185520079908\n",
      "Loss is  68.91027175819762\n",
      "Loss is  66.83699813222313\n",
      "Loss is  63.222461405097626\n",
      "Loss is  62.881289929093406\n",
      "Loss is  60.18750214584167\n",
      "Loss is  59.899468420387656\n",
      "Loss is  58.166031166794326\n",
      "Loss is  56.20822570998397\n",
      "Loss is  54.70439319944786\n",
      "Loss is  53.372152354337814\n",
      "Loss is  50.82357644701867\n",
      "Loss is  50.64070634278684\n",
      "Loss is  48.13589179251165\n",
      "Loss is  48.077837464758964\n",
      "Loss is  45.27880181434353\n",
      "Loss is  45.4006610115043\n",
      "Loss is  44.07337950126402\n",
      "Loss is  42.98023264266765\n",
      "Loss is  41.67764938896539\n",
      "Loss is  41.10192353682876\n",
      "Loss is  38.35802493696981\n",
      "Loss is  38.227688181175225\n",
      "Loss is  38.31774816203534\n",
      "Loss is  37.09325278107824\n",
      "Loss is  36.3247041534387\n",
      "Loss is  35.600691535684625\n",
      "Loss is  34.7054170683867\n",
      "Loss is  34.03669935968151\n",
      "Loss is  33.254772103308014\n",
      "Loss is  31.815549699376703\n",
      "Loss is  31.619611734131723\n",
      "Loss is  30.844452399365345\n",
      "Loss is  30.08228786025176\n",
      "Loss is  29.63723413243537\n",
      "Loss is  28.451348662828238\n",
      "Loss is  28.462805317997606\n",
      "Loss is  27.254894102390814\n",
      "Loss is  26.297972271145962\n",
      "Loss is  26.314579573476863\n",
      "Loss is  25.956340049183737\n",
      "Loss is  25.377045714349116\n",
      "Loss is  24.438924099965398\n",
      "Loss is  24.06276127188612\n",
      "Loss is  23.953826443512828\n",
      "Loss is  23.355746782804214\n",
      "Loss is  22.583436015530673\n",
      "Loss is  22.49539265934836\n",
      "Loss is  21.638215846493914\n",
      "Loss is  21.37570384750979\n",
      "Loss is  20.96016367697111\n",
      "Loss is  20.61131478151578\n",
      "Loss is  19.660776972608357\n",
      "Loss is  19.947036876830207\n",
      "Loss is  19.34900039282119\n",
      "Loss is  19.01523028288137\n",
      "Loss is  18.522167048278668\n",
      "Loss is  18.414479907208012\n",
      "Loss is  18.029633460337003\n",
      "Loss is  17.437285940353153\n",
      "Loss is  17.333111311483158\n",
      "Loss is  16.75988204697238\n",
      "Loss is  16.373884954203675\n",
      "Loss is  16.163678069536655\n",
      "Loss is  15.213521014300381\n",
      "Loss is  15.592407981326168\n",
      "Loss is  15.46805482007904\n",
      "Loss is  15.242563557103622\n",
      "Loss is  14.950833618690638\n",
      "Loss is  14.750673256571998\n",
      "Loss is  14.450618488100105\n",
      "Loss is  14.107864550537204\n",
      "Loss is  13.9322278706357\n",
      "Loss is  13.541852688696911\n",
      "Loss is  13.370706874600378\n",
      "Loss is  12.98715219277545\n",
      "Loss is  13.087074901627542\n",
      "Loss is  12.802781084917077\n",
      "Loss is  12.525613707210471\n",
      "Loss is  12.33767705901383\n",
      "Loss is  12.110369502301893\n",
      "Loss is  12.045467219833926\n",
      "Loss is  11.72920721005792\n",
      "Loss is  11.463003641028589\n",
      "Loss is  11.445369672166995\n",
      "Loss is  11.321537475016154\n",
      "Loss is  10.527251000938943\n",
      "Loss is  10.749920430908638\n",
      "Loss is  10.828126711192963\n",
      "Loss is  10.52249933513824\n",
      "Loss is  10.450541982895464\n",
      "Loss is  10.2574353732991\n",
      "Loss is  10.089581438925471\n",
      "Loss is  9.98939367570939\n",
      "Loss is  9.86958910842323\n",
      "Loss is  9.76682414807709\n",
      "Loss is  9.593130411625195\n",
      "Loss is  9.385907542818153\n",
      "Loss is  9.287333692139716\n",
      "Loss is  9.272584640335156\n",
      "Loss is  9.124422194732489\n",
      "Loss is  8.876094350290444\n",
      "Loss is  8.895395991008108\n",
      "Loss is  8.807808082362143\n",
      "Loss is  8.684582789708653\n",
      "Loss is  8.386118346140831\n",
      "Loss is  8.367810071942019\n",
      "Loss is  8.372774341113509\n",
      "Loss is  8.153061810409223\n",
      "Loss is  8.13935733460323\n",
      "Loss is  7.950904604516038\n",
      "Loss is  7.944868020665135\n",
      "Loss is  7.8101207740774345\n",
      "Loss is  7.6280781208915736\n",
      "Loss is  7.702848433786308\n",
      "Loss is  7.584964504404438\n",
      "Loss is  7.0844892633328564\n",
      "Loss is  7.38789344123247\n",
      "Loss is  7.368311331335221\n",
      "Loss is  6.8192075664315\n",
      "Loss is  7.178634476166669\n",
      "Loss is  7.105143987124601\n",
      "Loss is  7.052391901454471\n",
      "Loss is  6.960564548594977\n",
      "Loss is  6.9184486150352935\n",
      "Loss is  6.812982449281336\n",
      "Loss is  6.765434594442448\n",
      "Loss is  6.647589989635869\n",
      "Loss is  6.609171816133718\n",
      "Loss is  6.527596450208379\n",
      "Loss is  6.502649599153988\n",
      "Loss is  6.440996788383582\n",
      "Loss is  6.324904239781305\n",
      "Loss is  6.304373918260442\n",
      "Loss is  6.245689662925384\n",
      "Loss is  6.173134060693546\n",
      "Loss is  6.15151025413667\n",
      "Loss is  5.969943991593005\n",
      "Loss is  5.972533030690688\n",
      "Loss is  5.9608582922701965\n",
      "Loss is  5.737745451577963\n",
      "Loss is  5.824049604981612\n",
      "Loss is  5.811178332514222\n",
      "Loss is  5.720202260552821\n",
      "Loss is  5.3792007282741245\n",
      "Loss is  5.5993659970138845\n",
      "Loss is  5.5809514129791\n",
      "Loss is  5.557809563118875\n",
      "Loss is  5.5127024985196424\n",
      "Loss is  5.5194889825611195\n",
      "Loss is  5.432698238348456\n",
      "Loss is  5.107587494302822\n",
      "Loss is  5.362917210916601\n",
      "Loss is  5.3080533378603825\n",
      "Loss is  5.28461746995231\n",
      "Loss is  5.223523023880926\n",
      "Loss is  5.143067736656797\n",
      "Loss is  5.145214186920868\n",
      "Loss is  5.029410320831605\n",
      "Loss is  4.959243466140057\n"
     ]
    }
   ],
   "source": [
    "classifier = TwoLayerNNClassifier(150,k,epoch=200)\n",
    "classifier.initialise_parameters(X)\n",
    "classifier.train(X,Y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9910152740341419"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(X,Y,classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
